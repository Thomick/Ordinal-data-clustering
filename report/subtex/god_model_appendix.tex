\begin{thm}
    \label{thm:projection_appendix}
    Assuming that the prior distribution of $\mu$ is uniform over $\bbrack{1, m}$ and $\pi > \frac{1}{2}$, then \(\forall c \in \set{0, 1}^{m-1}\),
    \[\argmax_{k \in \bbrack{1, m}} \Pr(\mu = k | C = c) = \argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1\]
\end{thm}
\tm {Rappeler ce que $c_i$ et $Z_i$ signifient}
\tm {Expliquer l'histoire du prior (de quel point de vue: hypothèse sur la distribution de $\mu$, ou assumption de l'estimateur, ou autre chose)}

\begin{proof}
\begin{lemma}
    \[ \Pr(C[i] = c[i] | \mu < i) = (1 - c[i]) \pi + c[i] (1 - \pi) \]
    \[ \Pr(C[i] = c[i] | \mu \not< i) = c[i] \pi + (1 - c[i]) (1 - \pi) \]
\end{lemma}
\tm {Pas besoin de numéro si tu ne fais pas référence à une équation plus tard}
\begin{proof}
    \begin{align}
        \Pr(C[i] = c[i] | \mu \not< i)
        &= \Pr(C[i] = c[i] | Z[i] = 1, \mu \not< i) \Pr(Z[i] = 1) \\
        &\qquad + \Pr(C[i] = c[i] | Z[i] = 0, \mu \not< i) \Pr(Z[i] = 0)\\
                &= \II\{c[i]=1\} \Pr(Z[i] = 1) \\
        &\qquad + \II\{c[i]=0\} \Pr(Z[i] = 0)
    \end{align}
    Indeed conditional on $\mu \not< i$ and $Z[i] = 1$, $C[i] = 1$. Similarly, conditional on $\mu \not< i$ and $Z[i] = 0$, $C[i] = 0$. Hence:
    \begin{align}
        \ \Pr(C[i] = c[i] | \mu \not< i)
        &= c[i] \Pr(Z[i] = 1) + (1 - c[i]) \Pr(Z[i] = 0)\\
        &= c[i] \pi + (1 - c[i]) (1 - \pi)\\
    \end{align}
    \begin{align}
        \ \Pr(C[i] = c[i] | \mu < i)
        &= \Pr(C[i] = c[i] | Z[i] = 1, \mu < i) \Pr(Z[i] = 1) \\
        & \qquad + \Pr(C[i] = c[i] | Z[i] = 0, \mu < i) \Pr(Z[i] = 0)\\
        &= (1 - c[i]) \Pr(Z[i] = 1) + c[i] \Pr(Z[i] = 0)\\
        &= (1 - c[i]) \pi + c[i] (1 - \pi)
    \end{align}
\end{proof}

\begin{lemma}
    \label{lemma:p_c_mu}
    $\forall c \in \set{0, 1}^m, \forall k \in \bbrack{1, m}$,
    \[\Pr(C = c | \mu = k) = \pi^{m - 1 - \norm{c - E_k}_1} (1 - \pi)^{\norm{c - E_k}_1}\]
\end{lemma}
\begin{proof}
    Let us compute $\Pr(C = c| \mu = i)$ for $i \in \bbrack{1, m}$ by noticing that $C[i] | \mu$ are conditionally independent
\tm {Eclaircir les égalités (par exemple en deux temps séparé par un peu de texte)}
\tm {Preuve à corriger (inversion des inégalités entre mu et i)}

    \begin{align}
        \Pr(C = c| \mu = k)
        &= \prod_{i = 1}^{m -1} \Pr(C[i] = c[i] | \mu = k)\\
        &= \prod_{i = 1}^{k} \Pr(C[i] = c[i] | \mu \not< i ) \prod_{i = k+1}^{m-1} \Pr(C[i] = c[i] | \mu < i)\\
    \end{align}
    The last line come from the fact that $\Pr(C[i] = c[i])$ only depends on whether $\mu < i$ or not. Hence, if $i \leq k$, then $\Pr(C[i] = c[i] | \mu = k) = \Pr(C[i] = c[i] | \mu \not< i)$ and if $i > k$, then $\Pr(C[i] = c[i] | \mu = k) = \Pr(C[i] = c[i] | \mu<i)$. We now apply the previous lemma to finish the proof.
    \begin{align}
        \Pr(C = c| \mu = k)
        &= \prod_{i = 1}^{k-1} \Pr(C[i] = c[i] | \mu \not< i) \prod_{i = k}^{m-1} \Pr(C[i] = c[i] | \mu < i)\\
        &= \prod_{i = 1}^{k-1} [c[i] \pi + (1 - c[i]) (1 - \pi)] \prod_{i = k}^{m-1} [(1 - c[i]) \pi + c[i] (1 - \pi)]\\
        &= \pi^{\sum_{i = 1}^{k-1} c[i]} (1 - \pi)^{\sum_{i = 1}^{k-1} (1 - c[i])} \pi^{\sum_{i = k}^{m-1} (1 - c[i])} (1 - \pi)^{\sum_{i = k}^{m-1} c[i]}\\
        &= \pi^{\sum_{i = 1}^{k-1} c[i] + \sum_{i = k}^{m-1} (1 - c[i])} (1 - \pi)^{\sum_{i = 1}^{k-1} (1 - c[i]) + \sum_{i = k}^{m-1} c[i]} \\
        &= \pi^{m - 1 - \left[\sum_{i = 1}^{k-1} (1 - c[i]) + \sum_{i = k}^{m -1} c[i] \right]} (1 - \pi)^{\sum_{i = 1}^{k-1} (1 - c[i]) + \sum_{i = k}^{m-1} c[i]}\\
        &= \pi^{m - 1 - \norm{E_k - c}_1} (1 - \pi)^{\norm{E_k - c}_1}
    \end{align}
\end{proof}


\begin{align}
    \Pr(\mu = k | C = c) 
    &= \frac{\Pr(C = c | \mu = k) \Pr(\mu = k)}{\Pr(C=c)}\\
    &= \frac{\Pr(C = c | \mu = k) \Pr(\mu = k)}{\sum_{i = 1}^m \Pr(C = c | \mu = i) \Pr(\mu = i)}
\end{align}

As $\mu$ is uniformly distributed over $\bbrack{1, m}$, $\Pr(\mu = k) = \frac{1}{m}$

\begin{align}
    \Pr(\mu = k | C = c) 
    &= \frac{\Pr(C = c| \mu = k)}{\sum_{i = 1}^m \Pr(C | \mu = i)}
\end{align}

using Lemma~\ref{lemma:p_c_mu}:

\begin{align}
    \Pr(\mu = k | C = c)
    &= \frac{\pi^{m - 1 - \norm{c - E_k}_1} (1 - \pi)^{\norm{c - E_k}_1}}{\sum_{i = 1}^m \pi^{m - 1 - \norm{c - E_i}_1} (1 - \pi)^{\norm{c - E_i}_1}}\\
\end{align}

As $\pi > \frac{1}{2}$, we conclude that:
\[\argmax_{k \in \bbrack{1, m}} \Pr(\mu = k | C = c) = \argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1\]
\end{proof}


\begin{lemma}
    \label{lemma:p_x_c_knowing_pi_mu_appendix}
    \[\Pr(x, c | \pi, \mu) = \indic{}_{\mathcal{C}_x}(c) \pi^{m-1}  \frac{\left(\frac{1 - \pi}{\pi}\right)^{\norm{c - E_{\mu}}_1}}{\card{\argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1}} \]
\end{lemma}
\tm {dire c'est qui $C_x$}
\begin{proof}
    Using Bayes' theorem, then Lemma~\ref{lemma:p_c_mu} and the fact that $\mu$ is uniformly distributed over the set defined by the $\argmin$, we have:
    \begin{align}
        \Pr(x, C=c | \pi, \mu)
        &= \Pr(x | c, \pi, \mu) \Pr(C = c | \pi, \mu)\\
        &=  \indic{}_{\mathcal{C}_x}(c) \Pr(x | c \in \mathcal{C}_x, \pi, \mu) \Pr(c | \pi, \mu)\\
        &= \indic{}_{\mathcal{C}_x}(c) \frac{\pi^{m - 1 - \norm{c - E_{\mu}}_1} (1 - \pi)^{\norm{c - E_{\mu}}_1}}{\card{\argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1}}\\
        &= \indic{}_{\mathcal{C}_x}(c) \pi^{m-1}  \frac{\left(\frac{1 - \pi}{\pi}\right)^{\norm{c - E_{\mu}}_1}}{\card{\argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1}}
    \end{align}
\end{proof}


\begin{thm}[Observation likelihood]
    \label{thm:p_x_knowing_pi_mu}
    \[\Pr(x | \pi, \mu) = \pi^{m-1} \sum_{d = 0}^{m-1} \left(\frac{1 - \pi}{\pi}\right)^d u(x, \mu, d)\]
\end{thm}
\tm {dire c'est quoi $u(x, \mu, d)$}
\begin{proof}
    By marginalizing over $c$ and then using the previous lemma, we have:

\begin{align}
    \Pr(x | \pi, \mu)
    &= \sum_{c \in \set{0, 1}^{m-1}} \Pr(x, c | \pi, \mu)\\
    &= \pi^{m-1} \sum_{c \in \mathcal{C}_x} \left(\frac{1 - \pi}{\pi}\right)^{\norm{c - E_{\mu}}_1} \card{\argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1}^{-1}\\
    &= \pi^{m-1} \sum_{d = 0}^{m-1} \left(\frac{1 - \pi}{\pi}\right)^d \sum_{c \in \mathcal{C}_x / \norm{c - E_{\mu}}_1 = d}  \card{\argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1}^{-1}
\end{align}
\end{proof}



\begin{lemma}
    \label{lemma:cd_log_concave_compatible}
    We define for $d \in \NN$,
    \[ c_d: \begin{cases}
        \left[\frac{1}{2}, 1\right] &\rightarrow \RR_+^*\\
        x &\mapsto \left(\frac{1 - x}{x}\right)^d
    \end{cases}\]

    We have that $\forall d \in \NN, \forall x \in \left[\frac{1}{2}, 1\right]$:
    \[ c_d'(x)^2 - c_d(x) c_d''(x) \geq 0 \]
\end{lemma} 
\begin{proof}
    We have that:
    For $d \geq 1$:
    \begin{align}
        c_d'(x) &= -d x^{-2} \left(\frac{1 - x}{x}\right)^{d - 1}\\
        &= -d x^{-2} c_{d - 1}(x)
    \end{align}
    For $d \geq 2$:
    \begin{align}
        c_d''(x) 
        &= 2d x^{-3} \left(\frac{1 - x}{x}\right)^{d - 1} +  d(d-1) x^{-4} \left(\frac{1 - x}{x}\right)^{d - 2} \\
        &= d x^{-4} \left(\frac{1 - x}{x}\right)^{d - 2} \left(2 x\left(\frac{1 - x}{x}\right) + (d - 1)\right) \\
        &= d x^{-4} c_{d - 2}(x) \left(1 - 2x + d\right)
    \end{align}

    Therefore, we have that:
    \begin{itemize}
        \item For $d < 2$:
        \begin{equation}
            c_d'(x)^2 - c_d(x) c_d''(x) = c_d'(x)^2 \geq 0
        \end{equation}
        \item For $d \geq 2$:
        \begin{align}
            c_d'(x)^2 - c_d(x) c_d''(x) 
            &= d^2 x^{-4} \left(\frac{1 - x}{x}\right)^{2d - 2} - d x^{-4} \left(\frac{1 - x}{x}\right)^{2d - 2} \left(1 - 2x + d\right)\\
            &= d x^{-4} \left(\frac{1 - x}{x}\right)^{2d - 2} \left(d - 1 + 2x - d\right)\\
            &= d x^{-4} \left(\frac{1 - x}{x}\right)^{2d - 2} \left(2x - 1\right)\\
            &\geq 0 \qquad \text{since $2x - 1 \geq 0$ on $\left[\frac{1}{2}, 1\right]$}
        \end{align}
    \end{itemize}
    

    
\end{proof}


\begin{thm}
    \label{thm:log_likelihood_concave_appendix}
    $\forall \mu \in \bbrack{1, m}, \forall x \in \bbrack{1, m}$
    \[ \pi \mapsto \Pr(x | \pi, \mu) \]
    is $\log$-concave on $\left[\frac{1}{2}, 1\right]$.
\end{thm}

\begin{proof}
    We use the following expression:
    \[\log\Pr(x | \pi, \mu) = (m-1)\log \pi + \log\left[ \sum_{d = 0}^{m-1} \left(\frac{1 - \pi}{\pi}\right)^d u(x, \mu, d) \right] \] 

    As $\pi \mapsto (m-1) \log\pi$ is concave and the sum of positive weighted ($m - 1 \geq 0$) concave functions is concave, we only need to prove that $\ln g$ is concave where:
    \[g: t\mapsto \sum_{d=0}^{m-1} c_d(t) u_d \]
    As we will only use the fact that $u(x, \mu, d) \geq 0$ we replace the $u(x, \mu, d)$ by a generic $u_d$. 

    Using the lemma~\ref{lemma:concavity_log_composed_functions} we just have to check that $\forall t \in \left[\frac{1}{2}, 1\right], g'(t)^2 - g(t) g''(t) \geq 0$.

    Let $t \in  \left[\frac{1}{2}, 1\right]$. As $g$ is a positively weighted sum of $c_d$ and as each $c_d$ verify that $c_d'(t)^2 - c_d(t) c_d''(t) \geq 0$ we have $g'(t)^2 - g(t) g''(t) \geq 0$.
    
    We can conclued that $\Pr(x | \bullet, \mu)$ is $\log$-concave on $\left[\frac{1}{2}, 1\right]$.
\end{proof}
