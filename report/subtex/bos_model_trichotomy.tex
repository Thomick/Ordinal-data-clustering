\subsection{BOS Model}

\begin{definition}
    We define $\correct{\mu}{e}{y}{f}$ as the indicator function that $f$ is the correct subset to choose in case of a perfect comparison \textit{i.e.} if $\mu \in f$ or by default the closest to $\mu$.
\end{definition}

\begin{definition}
    We define $\nextes{e}{y}$ as the set of intervals that can be chosen after a comparison at breakpoint $y$ in the interval $e$ \textit{i.e.} $\nextes{e}{y} = \set{e^{-,y}, e^{=,y}, e^{+,y}}$ with $e = \bbrack{l, u - 1}$ and $y \in \bbrack{l, u - 1}$: $e^{-,y} = \bbrack{l, y - 1}$, $e^{=,y} = \set{y}$ and $e^{+,y} = \bbrack{y + 1, u - 1}$.
\end{definition}

We now suppose 
\begin{lemma}[$e_j$ transition]
    \label{lemma:bos_transition}
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}, \pi \in [0, 1], \forall e \subset \bbrack{1, m}, \forall f \subset e$:
    \[ \Pr(f | x \in e, e, \mu, \pi) =  \frac{1}{\card{e}} \sum_{y \in e} \left[ \correct{\mu}{e}{y}{f} \pi + \frac{\card{f}}{\card{e}} (1 - \pi) \right] \indickronecker{x \in \nextes{e}{y}} \]
\end{lemma}
\begin{proof}
    We have that, by marginalization over the breakpoint $y$:
    \begin{align}
        \Pr(f | x \in e, e, \mu, \pi) 
        &= \sum_{y \in e} \Pr(f, y | x \in e, e, \mu, \pi) \\
        &= \sum_{y \in e} \Pr(f | y, x \in e, e, \mu, \pi) \Pr(y | x \in e, e, \mu, \pi) \\
        &= \sum_{y \in e} \Pr(f | y, x \in e, e, \mu, \pi) \frac{1}{\card{e}} \\
        &= \frac{1}{\card{e}} \sum_{y \in e} \Pr(f | y, x \in e, e, \mu, \pi)
    \end{align}

    Then by marginalization over the accuracy indicator $z$:
    \begin{align}
        \Pr(f | x \in e, e, \mu, \pi) 
        &= \frac{1}{\card{e}} \sum_{y \in e} \sum_{z \in \set{0, 1}} \Pr(f | y, x \in e, e, \mu, \pi, z) \Pr(z | y, x \in e, e, \mu, \pi) \\
        &= \frac{1}{\card{e}} \sum_{y \in e} \left[ \Pr(f | y, x \in e, e, \mu, \pi, z=1) \pi + \Pr(f | y, x \in e, e, \mu, \pi, z=0) (1 - \pi) \right] \\
        &= \frac{1}{\card{e}}  \sum_{y \in e} \left[ \correct{\mu}{e}{y}{f} \pi + \frac{\card{f}}{\card{e}} (1 - \pi) \right]  \indickronecker{x \in \nextes{e}{y}}
    \end{align}

    Note that $\indickronecker{x \in \nextes{e}{y}} = 1$ only for one value of $y$ and $0$ for all the others.
\end{proof}

\begin{lemma}
    \label{lemma:bos_polynomial}
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}, \pi \in [0, 1], \forall e \subset \bbrack{1, m}$:
    \[ \pi \mapsto \Pr(x | x \in e, e, \mu, \pi)\] 
    is a polynomial function of degree at most $\card{e} - 1$.
\end{lemma}
\begin{proof}
    Let $m \in \NN^*, x \in \bbrack{1, m}, \mu \in \bbrack{1, m}, \pi \in [0, 1]$.

    We proceed by strong induction on $\card{e}$.
    \begin{itemize}
        \item Initialization: $\card{e} = 1$:
        \[ \Pr(x | x \in e, e, \mu, \pi) = \indickronecker{e = \set{x}} \] which is a polynomial function of degree $0$.
        
        \item Induction: Suppose the result holds for all $f \subset \bbrack{1, m}$ of size less or equal than $\card{e} - 1$ and let us prove it for $\card{e}$.
        
        We marginalize over the next interval $f$ and we have that:
        \begin{align}
            \Pr(x | x \in e, e, \mu, \pi) 
            &= \sum_{f \subset e} \Pr(x, f | x \in e, e, \mu, \pi) \\
            &= \sum_{f \subset e} \Pr(x | f, x \in e, e, \mu, \pi) \Pr(f | x \in e, e, \mu, \pi)
        \end{align}

        We can then notice that $\Pr(x | f, x \in e, e, \mu, \pi)$ is $0$ if $x \notin f$ and that $e$ does not intervene in the BOS process anymore. Hence we can replace $\Pr(x | f, x \in e, e, \mu, \pi)$ by $\Pr(x | x \in f, f, \mu, \pi)$ and sum only over $f \subset e$ such that $x \in f$:

        \begin{align}
            \Pr(x | x \in e, e, \mu, \pi) 
            &= \sum_{f \subset e ; x \in f} \Pr(x | x \in f, f, \mu, \pi) \Pr(f | x \in e, e, \mu, \pi)
        \end{align}

        As $\Pr(f | x \in e, e, \mu, \pi)$ is a polynomial function of degree at most $1$ (see lemma \ref{lemma:bos_transition}) and $\Pr(x | f, x \in f, f, \mu, \pi)$ is a polynomial function of degree at most $\card{f} - 1 \leq \card{e} - 2$ by induction hypothesis, we have that $\Pr(x | x \in e, e, \mu, \pi)$ is a polynomial function of degree at most $\card{e} - 1$.
    \end{itemize}

    Hence the result holds for all $e$.
\end{proof}


\begin{thm}[Likelihood is polynomial]
    \label{thm:likelihood_bos_is_polynomial}
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}$,:
    \[ \pi \mapsto \Pr(x | \mu, \pi) \]
    is a polynomial function of degree at most $m - 1$.  
\end{thm}
\begin{proof}
    Let $m \in \NN^*$, $x \in \bbrack{1, m}$ and $\mu \in \bbrack{1, m}$. 

    First we can introduce redondant knowledge as we start necessarily with the full set of categories, we can add its value as known. We have that $\Pr(x | \mu, \pi) = \Pr(x | e_1, \mu, \pi)$. We also now that $x \in e_1$ therefore $\Pr(x | \mu, \pi) = \Pr(x | x \in e_1, e_1, \mu, \pi)$.

    We can now use the previous lemma~\ref{lemma:bos_polynomial} to conclude that $\Pr(x | \mu, \pi)$ is a polynomial function of degree at most $m - 1$.
\end{proof}


We can now prove that $\forall x \in \bbrack{0, h - 1}, \forall \mu \in \bbrack{0, h - 1}, \pi \mapsto \Pr(x | x \in \bbrack{0, h - 1}, \mu, \pi)$ is concave on $[0, 1]$


\begin{lemma}[Log concavity affine times polynomial]
    \label{lemma:concavity_log_polynomial_times_affine}
    Let $P$ be a strictly $\log$-concave polynomial positive function on $I$ a real interval and $a, b \in \RR$ with $a \neq 0$ and $\forall x  \in I, ax + b \neq 0$. Then $f: x \mapsto (ax + b)P(x)$ is $\log$-concave on $I$.

    Moreover, if we just have that $P$ is $\log$-concave on $I$ without the assumption on $a$ and $b$, we have that $f$ is $\log$-concave on $I$.
\end{lemma}
\begin{proof}
    Using the lemma~\ref{lemma:concavity_log_composed_functions} we have that $P'(x)^2 - P(x)P''(x) > 0$.

    We have that:
    \begin{align}
        f'(x) &= aP(x) + (ax + b)P'(x) \\
        f''(x) &= 2 aP'(x) + (ax + b)P''(x) \\
        f'(x)^2 &= a^2P(x)^2 + 2a(ax + b)P(x)P'(x) + (ax + b)^2P'(x)^2 \\
        f(x)f''(x) &= 2a(ax + b) P(x)P'(x) + (ax + b)^2P(x)P''(x)
    \end{align}

    Hence:
    \[f'(x)^2 - f(x) f''(x) = a^2 P(x)^2 + (ax + b)^2 \left[ P'(x)^2 - P(x)P''(x) \right] \]

    As all the terms are strictly positive, we have that $\forall x \in I, f'(x)^2 - f(x) f''(x) > 0$ and using the lemma~\ref{lemma:concavity_log_composed_functions} we have that $f$ is strictly $\log$-concave on $I$.

    If we just have that $P$ is $\log$-concave on $I$ without the assumption on $a$ and $b$, we have that $\forall x \in I, f'(x)^2 - f(x) f''(x) \geq 0$ and using the lemma~\ref{lemma:concavity_log_composed_functions} we have that $f$ is $\log$-concave on $I$.
\end{proof}

\begin{lemma}[Log concavity of the BOS model]
    \label{lemma:log_concavity_bos_model}
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m},, \forall e \subset \bbrack{1, m}$:
    \[ \pi \mapsto \Pr(x | x \in e, e, \mu, \pi) \]
    is $\log$-concave on $[0, 1]$ and strictly $\log$-concave on $]0, 1[$.
\end{lemma}
\begin{proof}
    Let $m \in \NN^*$, $x \in \bbrack{1, m}$ and $\mu \in \bbrack{1, m}$.
    We proceed by induction on $\card{e}$.

    \begin{itemize}
        \item Initialization: $\card{e} = 1$:
        \[ \Pr(x | x \in e, e, \mu, \pi) = \indickronecker{e = \set{x}} \] which is constant and $\log$-concave.

        \item Induction: Suppose the result holds for all $f \subset \bbrack{1, m}$ of size less or equal than $\card{e} - 1$ and let us prove it for $\card{e}$.
        
        Using the lemma~\ref{lemma:bos_polynomial}, we have:
        \begin{align}
            \Pr(x | x \in e, e, \mu, \pi) 
            &= \sum_{f \subset e ; x \in f} \Pr(x | x \in f, f, \mu, \pi) \Pr(f | x \in e, e, \mu, \pi)
        \end{align}

        \tr{Need to finish the proof.}
    \end{itemize}

\end{proof}


\begin{thm}[Log concavity of the BOS model]
    $\forall x \in \bbrack{0, h - 1}, \forall \mu \in \bbrack{0, h - 1}, f: \pi \mapsto \Pr(x | x \in \bbrack{0, h - 1}, \mu, \pi)$ is $\log$-concave on $[0, 1]$ and a postive (for $\pi \in [0, 1]$) polynomial of degree less than $h - 1$.
\end{thm}
\begin{proof}
    We proceed by induction on $h$:

    Initialization: $h = 1$: 
    \[ \forall x \in \bbrack{0, h - 1}, \forall \mu \in \bbrack{0, h - 1}, \Pr(x | x \in \bbrack{0, h - 1}, \mu, \pi) = 1\] which is $\log$-concave and a positive polynomial of degree $0$.

    Induction: Suppose the theorem holds for $h - 1$ and let us prove it for $h$.

    Using the previous formula we have that $f$ is a sum of positive affine function in $\pi$ times $\Pr(x | x \in \bbrack{0, y - 1}, \mu, \pi)$ which is $\log$-concave by induction hypothesis and a positive polynomial of degree $y - 1$. We immediately deduce that $f$ is postive and polynomial of degree less than $h - 1$. Moreover using the previous lemma~\ref{lemma:concavity_log_polynomial_times_affine} we have that $f$ is $\log$-concave.

    Hence the theorem holds for $h$.
\end{proof}



\begin{align}
    \Pr(x | x \in e_j, \mu, \pi) 
    &= \sum_{e_{j+1} \subset e_j} \Pr(x, e_{j+1} | x \in e_j, \mu, \pi) \\
    &= \sum_{e_{j+1} \subset e_j} \Pr(x | e_{j+1}, x \in e_j, \mu, \pi) \Pr(e_{j+1} | e_j, \mu, \pi) \\
    &= \sum_{e_{j+1} \subset e_j ; x\in e_{j+1}} \Pr(x | x \in e_{j+1}, \mu, \pi) \Pr(e_{j+1} | e_j, \mu, \pi)
\end{align}

We now suppose $e_j = \bbrack{l, h - 1}$:

\begin{align}
    &\Pr(x | x \in \bbrack{l, h-1}, \mu, \pi) =\\
    &\begin{aligned}
        &\sum_{y=x + 1}^{h-1} \Pr(\bbrack{l, y - 1} | \bbrack{l, h-1}, \mu, \pi) \Pr(x | x \in \bbrack{l, y - 1}, \mu, \pi) \\
        &+ \Pr(\set{x} | \bbrack{l, h-1}, \mu, \pi) \Pr(x | x \in \set{x}, \mu, \pi) \\
        &+ \sum_{y=l}^{x - 1} \Pr(\bbrack{y + 1, h-1} | \bbrack{l, h-1}, \mu, \pi) \Pr(x | x \in \bbrack{y + 1, h-1}, \mu, \pi)
    \end{aligned} \\
    &= \begin{aligned}
        &\frac{1}{h - l} \sum_{y=x + 1}^{h-1} \left[ \pi \indickronecker{\mu < y} + (1-\pi) \frac{y - l}{h - l} \right] \Pr(x | x \in \bbrack{l, y - 1}, \mu, \pi) \\
        &+\frac{1}{h - l} \left[ \pi \indickronecker{\mu = x \lor (x = l \land \mu \leq x) \lor (x = h - 1 \land \mu \geq x)} + (1 - \pi) \frac{1}{h-l} \right] \\
        &\Pr(x | x \in \set{x}, \mu, \pi) \\
        &+\frac{1}{h - l} \sum_{y=l}^{x - 1} \left[ \pi \indickronecker{\mu > y} + (1-\pi) \frac{h - y - 1}{h - l} \right] \Pr(x | x \in \bbrack{y + 1, h-1}, \mu, \pi)
    \end{aligned}
\end{align}

As $\Pr(x | x \in \set{x}, \mu, \pi) = 1$ this allows to compute the probability of $x$ being in the interval $\bbrack{l, h-1}$ recursively.

As:
\begin{equation}
    \Pr(x | x \in \bbrack{l, y - 1}, \mu, \pi) = \Pr(x - l | x - l \in \bbrack{0, y - l - 1}, \max(0, \mu - l), \pi)
\end{equation}

We can rewrite the previous equation as:

\begin{align}
    h\Pr(x | x \in \bbrack{0, h - 1})
    &= \sum_{y = x + 1}^{h - 1} \left[ \pi \indickronecker{\mu < y} + (1-\pi) \frac{y}{h} \right] \Pr(x | x \in \bbrack{0, y - 1}, \mu, \pi) \\
    &+ \pi \indickronecker{\mu = x \lor (x = 0 \land \mu \leq x) \lor (x = h - 1 \land \mu \geq x)} + (1 - \pi) \frac{1}{h} \\
    &+ \sum_{y = 0}^{x - 1} \left[ \pi \indickronecker{\mu > y} + (1-\pi) \frac{h - y - 1}{h} \right] \Pr(x - y - 1 | x - y - 1 \in \bbrack{0, h - y - 2}, \max(0, \mu - y - 1), \pi)
\end{align}



