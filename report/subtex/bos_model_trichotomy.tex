\subsection{BOS Model}

\begin{definition}
    We define $\correct{\mu}{e}{y}{f}$ as the indicator function that $f$ is the correct subset to choose in case of a perfect comparison \textit{i.e.} if $\mu \in f$ or by default the closest to $\mu$.
\end{definition}
\begin{proof}
    
    
\end{proof}


We now suppose 
\begin{lemma}[$e_j$ transition]
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}, \pi \in [0, 1], \forall e \subset \bbrack{1, m}, \forall f \subset e$:
    \[ \Pr(f | x \in e, e, \mu, \pi) =  \frac{1}{\card{e}} \sum_{y \in e} \left[ \correct{\mu}{e}{y}{f} \pi + \frac{\card{f}}{\card{e}} (1 - \pi) \right] \]
\end{lemma}
\begin{proof}
    We have that, by marginalization over the breakpoint $y$:
    \begin{align}
        \Pr(f | x \in e, e, \mu, \pi) 
        &= \sum_{y \in e} \Pr(f, y | x \in e, e, \mu, \pi) \\
        &= \sum_{y \in e} \Pr(f | y, x \in e, e, \mu, \pi) \Pr(y | x \in e, e, \mu, \pi) \\
        &= \sum_{y \in e} \Pr(f | y, x \in e, e, \mu, \pi) \frac{1}{\card{e}} \\
        &= \frac{1}{\card{e}} \sum_{y \in e} \Pr(f | y, x \in e, e, \mu, \pi)
    \end{align}

    Then by marginalization over the accuracy indicator $z$:
    \begin{align}
        \Pr(f | x \in e, e, \mu, \pi) 
        &= \frac{1}{\card{e}} \sum_{y \in e} \sum_{z \in \set{0, 1}} \Pr(f | y, x \in e, e, \mu, \pi, z) \Pr(z | y, x \in e, e, \mu, \pi) \\
        &= \frac{1}{\card{e}} \sum_{y \in e} \left[ \Pr(f | y, x \in e, e, \mu, \pi, z=1) \pi + \Pr(f | y, x \in e, e, \mu, \pi, z=0) (1 - \pi) \right] \\
        &= \frac{1}{\card{e}} \sum_{y \in e} \left[ \correct{\mu}{e}{y}{f} \pi + \frac{\card{f}}{\card{e}} (1 - \pi) \right]
    \end{align}

\end{proof}


\begin{thm}[likelihood is polynomial]
    \label{thm:likelihood_bos_is_polynomial}
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}$,:
    \[ \pi \mapsto \Pr(x | \mu, \pi) \]
    is a polynomial function of degree at most $m - 1$.  
\end{thm}
\begin{proof}
    Le $m \in \NN^*$, $x \in \bbrack{1, m}$ and $\mu \in \bbrack{1, m}$. 

    First we can introduce redondant knowledge as we start necessarily with the full set of categories, we can add its value as known. We have that $\Pr(x | \mu, \pi) = \Pr(x | e_1, \mu, \pi)$. We also now that $x \in e_1$ therefore $\Pr(x | \mu, \pi) = \Pr(x | x \in e_1, e_1, \mu, \pi)$.

    We now prove the following $\forall e \subset \bbrack{1, m}, \pi \mapsto \Pr(x | x \in e_j, e_j, \mu, \pi)$ is a polynomial function of degree at most $\card{e_j} - 1$ by induction on $\card{e_j}$.

    Initialization: $\card{e_j} = 1$:
    \[ \Pr(x | x \in \set{x}, \set{x}, \mu, \pi) = 1 \] which is a polynomial function of degree $0$.

    Induction: Suppose the theorem holds for $\card{e_j} - 1$ and let us prove it for $\card{e_j}$.

    We have that:
    \begin{align}
        \Pr(x | x \in e_j, e_j, \mu, \pi) 
        &= \sum_{e_{j+1} \subset e_j} \Pr(x, e_{j+1} | x \in e_j, e_j, \mu, \pi) \\
        &= \sum_{e_{j+1} \subset e_j} \Pr(x | e_{j+1}, x \in e_j, e_j, \mu, \pi) \Pr(e_{j+1} | x \in e_j, e_j, \mu, \pi)
    \end{align}

    As $\Pr(e_{j+1} | x \in e_j, e_j, \mu, \pi)$ is a polynomial function of degree at most $1$ by induction hypothesis and $\Pr(x | e_{j+1}, x \in e_j, e_j, \mu, \pi)$ is a polynomial function of degree at most $\card{e_{j+1}} - 1 \leq \card{e_j} - 1$ by induction hypothesis, we have that $\Pr(x | x \in e_j, e_j, \mu, \pi)$ is a polynomial function of degree at most $\card{e_j} - 1$. 
\end{proof}


\begin{align}
    \Pr(x | x \in e_j, \mu, \pi) 
    &= \sum_{e_{j+1} \subset e_j} \Pr(x, e_{j+1} | x \in e_j, \mu, \pi) \\
    &= \sum_{e_{j+1} \subset e_j} \Pr(x | e_{j+1}, x \in e_j, \mu, \pi) \Pr(e_{j+1} | e_j, \mu, \pi) \\
    &= \sum_{e_{j+1} \subset e_j ; x\in e_{j+1}} \Pr(x | x \in e_{j+1}, \mu, \pi) \Pr(e_{j+1} | e_j, \mu, \pi)
\end{align}

We now suppose $e_j = \bbrack{l, h - 1}$:

\begin{align}
    &\Pr(x | x \in \bbrack{l, h-1}, \mu, \pi) =\\
    &\begin{aligned}
        &\sum_{y=x + 1}^{h-1} \Pr(\bbrack{l, y - 1} | \bbrack{l, h-1}, \mu, \pi) \Pr(x | x \in \bbrack{l, y - 1}, \mu, \pi) \\
        &+ \Pr(\set{x} | \bbrack{l, h-1}, \mu, \pi) \Pr(x | x \in \set{x}, \mu, \pi) \\
        &+ \sum_{y=l}^{x - 1} \Pr(\bbrack{y + 1, h-1} | \bbrack{l, h-1}, \mu, \pi) \Pr(x | x \in \bbrack{y + 1, h-1}, \mu, \pi)
    \end{aligned} \\
    &= \begin{aligned}
        &\frac{1}{h - l} \sum_{y=x + 1}^{h-1} \left[ \pi \indickronecker{\mu < y} + (1-\pi) \frac{y - l}{h - l} \right] \Pr(x | x \in \bbrack{l, y - 1}, \mu, \pi) \\
        &+\frac{1}{h - l} \left[ \pi \indickronecker{\mu = x \lor (x = l \land \mu \leq x) \lor (x = h - 1 \land \mu \geq x)} + (1 - \pi) \frac{1}{h-l} \right] \\
        &\Pr(x | x \in \set{x}, \mu, \pi) \\
        &+\frac{1}{h - l} \sum_{y=l}^{x - 1} \left[ \pi \indickronecker{\mu > y} + (1-\pi) \frac{h - y - 1}{h - l} \right] \Pr(x | x \in \bbrack{y + 1, h-1}, \mu, \pi)
    \end{aligned}
\end{align}

As $\Pr(x | x \in \set{x}, \mu, \pi) = 1$ this allows to compute the probability of $x$ being in the interval $\bbrack{l, h-1}$ recursively.

As:
\begin{equation}
    \Pr(x | x \in \bbrack{l, y - 1}, \mu, \pi) = \Pr(x - l | x - l \in \bbrack{0, y - l - 1}, \max(0, \mu - l), \pi)
\end{equation}

We can rewrite the previous equation as:

\begin{align}
    h\Pr(x | x \in \bbrack{0, h - 1})
    &= \sum_{y = x + 1}^{h - 1} \left[ \pi \indickronecker{\mu < y} + (1-\pi) \frac{y}{h} \right] \Pr(x | x \in \bbrack{0, y - 1}, \mu, \pi) \\
    &+ \pi \indickronecker{\mu = x \lor (x = 0 \land \mu \leq x) \lor (x = h - 1 \land \mu \geq x)} + (1 - \pi) \frac{1}{h} \\
    &+ \sum_{y = 0}^{x - 1} \left[ \pi \indickronecker{\mu > y} + (1-\pi) \frac{h - y - 1}{h} \right] \Pr(x - y - 1 | x - y - 1 \in \bbrack{0, h - y - 2}, \max(0, \mu - y - 1), \pi)
\end{align}


We can now prove that $\forall x \in \bbrack{0, h - 1}, \forall \mu \in \bbrack{0, h - 1}, \pi \mapsto \Pr(x | x \in \bbrack{0, h - 1}, \mu, \pi)$ is concave on $[0, 1]$


\begin{lemma}[Log concavity affine times polynomial]
    \label{lemma:concavity_log_polynomial_times_affine}
    Let $P$ a $\log$-concave polynomial postive polynomail (for all $x$ considered) and $a, b \in \RR$ with $ax + b \geq 0$. Then $f: x \mapsto (ax + b)P(x)$ is $\log$-concave.
\end{lemma}
\begin{proof}
    Using the lemma~\ref{lemma:concavity_log_composed_functions} we have that $P'(x)^2 - P(x)P''(x) \geq 0$.
    
    As
    \[f'(x)^2 - f(x) f''(x) = a^2 P(x)^2 + (ax + b) \left[ P'(x)^2 - P(x)P''(x) \right] \] 
    we have that $f'(x)^2 - f(x) f''(x) \geq 0$ hence using the lemma~\ref{lemma:concavity_log_composed_functions} we have that $f$ is $\log$-concave.
\end{proof}


\begin{thm}[Log concavity of the BOS model]
    $\forall x \in \bbrack{0, h - 1}, \forall \mu \in \bbrack{0, h - 1}, f: \pi \mapsto \Pr(x | x \in \bbrack{0, h - 1}, \mu, \pi)$ is $\log$-concave on $[0, 1]$ and a postive (for $\pi \in [0, 1]$) polynomial of degree less than $h - 1$.
\end{thm}
\begin{proof}
    We proceed by induction on $h$:

    Initialization: $h = 1$: 
    \[ \forall x \in \bbrack{0, h - 1}, \forall \mu \in \bbrack{0, h - 1}, \Pr(x | x \in \bbrack{0, h - 1}, \mu, \pi) = 1\] which is $\log$-concave and a positive polynomial of degree $0$.

    Induction: Suppose the theorem holds for $h - 1$ and let us prove it for $h$.

    Using the previous formula we have that $f$ is a sum of psotive affine function in $\pi$ times $\Pr(x | x \in \bbrack{0, y - 1}, \mu, \pi)$ which is $\log$-concave by induction hypothesis and a positive polynomial of degree $y - 1$. We immediately deduce that $f$ is postive and polynomial of degree less than $h - 1$. Moreover using the previous lemma~\ref{lemma:concavity_log_polynomial_times_affine} we have that $f$ is $\log$-concave.

    Hence the theorem holds for $h$.
\end{proof}
