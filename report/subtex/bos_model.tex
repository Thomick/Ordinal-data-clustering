
\subsection{Stochastic Binary Ordinal Search} 

The BOS model is inspired by a standard binary search with added noise in the comparison. Consequently, the algorithm may at times misidentify the next subset for the search, ultimately causing it to overlook the sought-after value.


\subsubsection{Probabilistic model}

The stochastic binary ordinal search unfolds as follows: Let $m$ be the number of categories. Then, for at most $m-1$ steps, we perform the following three operations.
We start with the full set of categories, denoted as $e_1 = \bbrack{1,m}$. Then we perform the following steps:

At step $j$, we start with a subset of all the categories, denoted as $e_j = \bbrack{l_j, u_j - 1} \subseteq \bbrack{1,m}$.

\begin{enumerate}
    \item Sample a breakpoint $y_j$ uniformly in $e_j$ ($y_j \sim \mathcal{U}(e_j)$).
    \item Draw an accuracy indicator $z_j$ from a Bernoulli distribution with parameter $\pi$ ($z_j \sim \text{Bernoulli}(\pi)$).
    A value of $z_j=1$ indicates that the comparison is perfect, and the next step will be computed optimally. A value of $z_j=0$ implies a blind comparison at the next step.
    \item Determine the new subset $e_{j+1}$ for the next iteration. Firstly, split the subset into three intervals, namely $e_j^- = \bbrack{l_j, y_j - 1}$, $e_j^= = \set{y_j}$, and $e_j^+ = \bbrack{y_j + 1, u_j - 1}$. $e_{j+1}$ will be chosen among these intervals. If the comparison is blind ($z_j=0$), randomly select the interval with a probability proportional to its size. Alternatively, if $z_j=1$ and the comparison is perfect, select the interval containing $\mu$ (or, by default, the one closest to it).
\end{enumerate}
After $m-1$ steps, the resulting interval contains a single value, which is the observed result $e_m=\set{x}$ of the BOS model.


\begin{figure}[htbp]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tikzpicture}
  \node[obs]                               (xi) {$x_i$};
  \node[latent, left=of xi]               (em) {$e_m$};
  \node[latent, below=of em]               (zm1) {$z_{m-1}$};
  \node[latent, left=of em]               (ym1) {$y_{m-1}$};
  \node[const, left=of ym1]               (dots2) {$\ldots$};
  \node[latent, left=of dots2]               (ej) {$e_{j}$};
  \node[latent, below=of ej]               (zj) {$z_{j}$};
  \node[latent, left=of ej]               (yj) {$y_{j}$};
  \node[const, left=of yj]               (dots1) {$\ldots$};
  \node[latent, left=of dots1]               (e2) {$e_2$};
  \node[latent, below=of e2]               (z1) {$z_1$};
  \node[latent, left=of e2]               (y1) {$y_1$};
  \node[latent, left=of y1]               (e1) {$e_1$};
  \node[const, above=of ej]    (mu) {$\mu$};
  \node[const, below=of zj, yshift=-.25cm]  (pi) {$\pi$};

  \edge{e1}{y1};
  \edge{y1}{e2};
  \edge{z1}{e2};
  \edge{e2}{dots1};
  \edge{dots1}{yj};
  \edge{yj}{ej};
  \edge{zj}{ej};
  \edge{ej}{dots2};
  \edge{dots2}{ym1};
  \edge{ym1}{em};
  \edge{zm1}{em};
  \edge{em}{xi};
  \edge {mu} {e2, ej, em};
  \edge {pi} {z1,zj, zm1};

  \plate[inner sep=.25cm] {} {(xi)(e1)(zm1)(z1)} {$i=1, \ldots, n$};
\end{tikzpicture}
\end{adjustbox}
\caption{Graphical model of the stochastic Binary Ordinal Search.}
\label{fig:graphical_model}
\end{figure}

%\ar{Do a figure for the Mixture case}
%\tm{Maybe, although it does not give much more information since it can be trivially derived from this one. Good job for the figure by the way}
%\ar{Agreed, and probably too complicated to do anyways}


\subsection{Parameter estimation}

\paragraph{EM algorithm.} In the univariate case, the underlying BOS distribution of a data sample $(x_1, \ldots, x_n)$ can be estimated using the Expectation-Maximization (EM) algorithm. The main idea behind EM is to iteratively compute the parameters $(\mu, \pi)$ of the distribution to maximize the log-likelihood until convergence when the model is explained by latent variables that are not observed with the data as it is the case with the BOS distribution \citep{biernacki2016model}.
\begin{enumerate}
    \item Initialization: The parameters of the distribution are initialized using a pre-defined heuristic. In our case, we initialize $(\mu^{(0)}, \pi^{(0)})$ randomly in their respective range.
    \item Expectation step: The latent variables $(c_i)_{i \in \{1, \ldots, n\}}$ are estimated using the current approximation of the parameters $(\mu^{(t)}, \pi^{(t)})$. This consists in computing the posterior probability of every latent variable using the conditional probability formula for all $i\in \{1, \ldots, ..., n\}$
    \begin{equation}
        \mathbb{P}(c_i | x_i, \mu^{(t)}, \pi^{(t)}) = \frac{\mathbb{P}(c_i, x_i| \mu^{(t)}, \pi^{(t)})}{\mathbb{P}(x_i| \mu^{(t)}, \pi^{(t)})}
    .\end{equation}
    This can be computed recursively for the different latent variables using their prior distribution (BOS model):
    \begin{equation}
        \mathbb{P}(e_{j+1}| \mu^{(t)}, \pi^{(t)}) = \sum_{e\in \mathcal{P}(\{1, \ldots, m\})} \mathbb{P}(e_{j+1}|e_j=e,  \mu^{(t)}, \pi^{(t)}) \mathbb{P}(e_j=e| \mu^{(t)}, \pi^{(t)})
    .\end{equation}
    The intermediate conditional probability is obtained by conditioning on the different values that can be taken by $y_j$ and $z_j$ when $e_j=e$:
    \begin{align}
    \mathbb{P}(e_{j+1}|e_j=e, \mu^{(t)}, \pi^{(t)}) &= \sum_{y_j \in e} \mathbb{P}(e_{j+1}|e_j=e, y_j, \mu^{(t)}, \pi^{(t)})\mathbb{P}(y_j|e_j=e) \\
    &\begin{aligned}
        = &\sum_{y_j \in e} (\pi^{(t)}\mathbb{P}(e_{j+1}|e_j, y_j, z_j=1, \mu^{(t)}, \pi^{(t)}) \\ 
        & + (1-\pi^{(t)})\mathbb{P}(e_{j+1}|e_j, y_j, z_j=0, \mu^{(t)}, \pi^{(t)}))\mathbb{P}(y_j|e_j=e)
    .\end{aligned}
    \end{align}
    The components of the sum can be more easily computed by distinguishing between the cases where $z_j=0$ and $z_j=1$ with closed-forms expressions from the prior. \\
    Moreover, for $y_j$, the posterior distribution can also be obtained using the following expression where we also condition on $e_j$:
    \begin{equation}
    \mathbb{P}(y_j| \mu^{(t)}, \pi^{(t)}) = \sum_{e\in \mathcal{P}(\{1, \ldots, m\}} \mathbb{P}(y_j|e_j=e, \mu^{(t)}, \pi^{(t)})\mathbb{P}(e_j=e| \mu^{(t)}, \pi^{(t)})
    .\end{equation}
    For $z_j$, $\mathbb{P}(z_j| \mu^{(t)}, \pi^{(t)})$ is a Bernoulli variable of probability $\pi^{(t)}$ so it can easily be expressed:
    \begin{equation}
    \mathbb{P}(z_j| \mu^{(t)}, \pi^{(t)}) = \begin{cases}
        \pi^{(t)}\quad &\text{if } z_j = 1\\
        1 - \pi^{(t)}\quad &\text{if } z_j = 0
        \end{cases}
    .\end{equation}
    %\ar{How does this allow to derive $\mathbb{P}(c_{ij}, x_i| \mu, \pi)?$. Ok}
    Note that since in the BOS model $e_m$ is identified with $x_i$, we get the following joint distribution for all the latent variables $c_i$ with the observation $x_i$: 
    \begin{equation}
       \mathbb{P}(c_i,x_i| \mu^{(t)}, \pi^{(t)}) = \mathbb{P}(e_m|c_i, \mu^{(t)}, \pi^{(t)})\mathbb{P}(c_i| \mu^{(t)}, \pi^{(t)})
    .\end{equation}
    \item Maximization step: During this step, the next iteration of the parameters $(\mu^{(t+1)}, \pi^{(t+1)})$ that maximizes the log-likelihood of observing the data taking into account the latent-variables probabilities computed during the Expectation step. As proposed by \citet{biernacki2016model}, only $\pi$ is updated and $\mu$ is fixed during the entire algorithm. This is done for every possible value of $\mu$ and the value that maximizes the log-likelihood is chosen. \\
    The new value of $\pi$ is given after computing the maximizer of the expectation of the log-likelihood in $\pi$:
    \begin{equation}
    \pi^{(t+1)} = \frac{\sum_{i=1}^N \sum_{j=1}^{m-1} \mathbb{P}(z_{ij}=1|x_i, \mu^{(t)}, \pi^{(t)})}{n(m-1)}
    .\end{equation}
    % \begin{proof}
    % \ar{Add the proof for this formula?}
    % \tm{Yes. I need to check it but not sure there is much to it. Th√©o opinion ?}
    % \end{proof}
    \footnote{The authors of the paper did not mention how they computed this formula. We computed it by marginalizing on every possible run of the binary search algorithm, leading to high computational cost.

    We use the following expression:
    \begin{align}
        \Pr(z_{ij}=1|x_i, \mu, \pi) &= \sum_{c_i} \Pr(z_{ij}=1, c_i|x_i, \mu, \pi) \\
        &= \sum_{c_i} \Pr(z_{ij}=1|c_i, x_i, \mu, \pi)\Pr(c_i|x_i, \mu, \pi) \\
        &= \sum_{c_i} \Pr(z_{ij}=1|c_i) \frac{\Pr(c_i, x_i|\mu, \pi)}{\Pr(x_i|\mu, \pi)} \\
        &= \frac{1}{\Pr(x_i|\mu, \pi)} \sum_{c_i} \Pr(z_{ij}=1|c_i) \Pr(c_i, x_i|\mu, \pi) 
    \end{align}
    And then:
    \begin{align}
        \sum_{j=1}^{m-1} \Pr(z_{ij}=1|x_i, \mu, \pi) &= \sum_{j=1}^{m-1} \frac{1}{\Pr(x_i|\mu, \pi)}  \sum_{c_i} \Pr(z_{ij}=1|c_i) \Pr(c_i, x_i|\mu, \pi) \\
        &= \frac{1}{\Pr(x_i|\mu, \pi)} \sum_{c_i} \Pr(c_i, x_i|\mu, \pi)  \sum_{j=1}^{m-1} \Pr(z_{ij}=1|c_i)
    \end{align}
    }

\end{enumerate}

