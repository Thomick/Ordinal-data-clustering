\section{BOS Model proofs}
\label{appendix:bos_proofs}

\subsection{Notations}

For the whole section we will consider that $e$ is a subset of $\bbrack{1, m}$ and that supposing that we know $e$ means that we look a the random process where the starting set of categories is $e$. We will also note $e^{-,y}$, $e^{=,y}$ and $e^{+,y}$ the sets of categories that are respectively less than, equal to and greater than $y$ in $e$ and $f$ any next set of categories considered in the BOS process. For example,  $\Pr(f | e)$ is the probability of having as next set of categories $f$ knowing that the current set of categories is $e$ (you could imagine that we have $j$ such that $e_j = e$ and $e_{j+1} = f$).

\begin{definition}
    We define $\correct{\mu}{e}{y}{f}$ as the indicator function that $f$ is the correct subset to choose in case of a perfect comparison \textit{i.e.} if $\mu \in f$ or by default the closest to $\mu$.
\end{definition}

\begin{definition}
    We define $\nextes{e}{y}$ as the set of intervals that can be chosen after a comparison at breakpoint $y$ in the interval $e$ \textit{i.e.} $\nextes{e}{y} = \set{e^{-,y}, e^{=,y}, e^{+,y}}$ with $e = \bbrack{l, u - 1}$ and $y \in \bbrack{l, u - 1}$: $e^{-,y} = \bbrack{l, y - 1}$, $e^{=,y} = \set{y}$ and $e^{+,y} = \bbrack{y + 1, u - 1}$.
\end{definition}


\subsection{Polynomiality}

\begin{lemma}[Transition probability]
    \label{lemma:bos_transition}
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}, \pi \in [0, 1], \forall e \subset \bbrack{1, m}, \forall f \subset e$:
    \[ \Pr(f | x \in e, e, \mu, \pi) =  \frac{1}{\card{e}}  \sum_{y \in e} \left[ \left( \correct{\mu}{e}{y}{f} - \frac{\card{f}}{\card{e}} \right) \pi + \frac{\card{f}}{\card{e}} \right]  \indickronecker{x \in \nextes{e}{y}} \]

    Note that $\indickronecker{x \in \nextes{e}{y}} = 1$ only for one value of $y$ and $0$ for all the others. Moreover $\pi \mapsto \Pr(f | x \in e, e, \mu, \pi)$ is an affine function.
\end{lemma}
\begin{proof}
    We have that, by marginalization over the breakpoint $y$:
    \begin{align}
        \Pr(f | x \in e, e, \mu, \pi) 
        &= \sum_{y \in e} \Pr(f, y | x \in e, e, \mu, \pi) \\
        &= \sum_{y \in e} \Pr(f | y, x \in e, e, \mu, \pi) \Pr(y | x \in e, e, \mu, \pi) \\
        &= \sum_{y \in e} \Pr(f | y, x \in e, e, \mu, \pi) \frac{1}{\card{e}} \\
        &= \frac{1}{\card{e}} \sum_{y \in e} \Pr(f | y, x \in e, e, \mu, \pi)
    \end{align}

    Then by marginalization over the accuracy indicator $z$:
    \begin{align}
        \Pr(f | x \in e, e, \mu, \pi) 
        &= \frac{1}{\card{e}} \sum_{y \in e} \sum_{z \in \set{0, 1}} \Pr(f | y, x \in e, e, \mu, \pi, z) \Pr(z | y, x \in e, e, \mu, \pi) \\
        &= \frac{1}{\card{e}} \sum_{y \in e} \left[ \Pr(f | y, x \in e, e, \mu, \pi, z=1) \pi + \Pr(f | y, x \in e, e, \mu, \pi, z=0) (1 - \pi) \right] \\
        &= \frac{1}{\card{e}}  \sum_{y \in e} \left[ \correct{\mu}{e}{y}{f} \pi + \frac{\card{f}}{\card{e}} (1 - \pi) \right]  \indickronecker{x \in \nextes{e}{y}} \\
        &= \frac{1}{\card{e}}  \sum_{y \in e} \left[ \left( \correct{\mu}{e}{y}{f} - \frac{\card{f}}{\card{e}} \right) \pi + \frac{\card{f}}{\card{e}} \right]  \indickronecker{x \in \nextes{e}{y}}
    \end{align}
\end{proof}

\begin{lemma}
    \label{lemma:bos_polynomial}
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}, \pi \in [0, 1], \forall e \subset \bbrack{1, m}$:
    \[ \pi \mapsto \Pr(x | x \in e, e, \mu, \pi)\] 
    is a polynomial function of degree at most $\card{e} - 1$.
\end{lemma}
\begin{proof}
    Let $m \in \NN^*, x \in \bbrack{1, m}, \mu \in \bbrack{1, m}, \pi \in [0, 1]$.

    We proceed by strong induction on $\card{e}$.
    \begin{itemize}
        \item Initialization: $\card{e} = 1$:
        \[ \Pr(x | x \in e, e, \mu, \pi) = \indickronecker{e = \set{x}} \] which is a polynomial function of degree $0$.
        
        \item Induction: Suppose the result holds for all $f \subset \bbrack{1, m}$ of size less or equal than $\card{e} - 1$ and let us prove it for $\card{e}$.
        
        We marginalize over the next interval $f$ and we have that:
        \begin{align}
            \Pr(x | x \in e, e, \mu, \pi) 
            &= \sum_{f \subset e} \Pr(x, f | x \in e, e, \mu, \pi) \\
            &= \sum_{f \subset e} \Pr(x | f, x \in e, e, \mu, \pi) \Pr(f | x \in e, e, \mu, \pi)
        \end{align}

        We can then notice that $\Pr(x | f, x \in e, e, \mu, \pi)$ is $0$ if $x \notin f$ and that $e$ does not intervene in the BOS process anymore. Hence we can replace $\Pr(x | f, x \in e, e, \mu, \pi)$ by $\Pr(x | x \in f, f, \mu, \pi)$ and sum only over $f \subset e$ such that $x \in f$:

        \begin{align}
            \Pr(x | x \in e, e, \mu, \pi) 
            &= \sum_{f \subset e ; x \in f} \Pr(x | x \in f, f, \mu, \pi) \Pr(f | x \in e, e, \mu, \pi)
        \end{align}

        As $\Pr(f | x \in e, e, \mu, \pi)$ is a polynomial function of degree at most $1$ (see lemma \ref{lemma:bos_transition}) and $\Pr(x | f, x \in f, f, \mu, \pi)$ is a polynomial function of degree at most $\card{f} - 1 \leq \card{e} - 2$ by induction hypothesis, we have that $\Pr(x | x \in e, e, \mu, \pi)$ is a polynomial function of degree at most $\card{e} - 1$.
    \end{itemize}

    Hence the result holds for all $e$.
\end{proof}


\begin{thm}[Likelihood is polynomial]
    \label{thm:likelihood_bos_is_polynomial}
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}$,:
    \[ \pi \mapsto \Pr(x | \mu, \pi) \]
    is a polynomial function of degree at most $m - 1$.  
\end{thm}
\begin{proof}
    Let $m \in \NN^*$, $x \in \bbrack{1, m}$ and $\mu \in \bbrack{1, m}$. 

    First we can introduce redondant knowledge as we start necessarily with the full set of categories, we can add its value as known. We have that $\Pr(x | \mu, \pi) = \Pr(x | e_1, \mu, \pi)$. We also now that $x \in e_1$ therefore $\Pr(x | \mu, \pi) = \Pr(x | x \in e_1, e_1, \mu, \pi)$.

    We can now use the previous lemma~\ref{lemma:bos_polynomial} to conclude that $\Pr(x | \mu, \pi)$ is a polynomial function of degree at most $m - 1$.
\end{proof}


\subsection{Concavity}

% We can now prove that $\forall x \in \bbrack{0, h - 1}, \forall \mu \in \bbrack{0, h - 1}, \pi \mapsto \Pr(x | x \in \bbrack{0, h - 1}, \mu, \pi)$ is concave on $[0, 1]$
% 
% 
% \begin{lemma}[Log concavity affine times polynomial]
%     \label{lemma:concavity_log_polynomial_times_affine}
%     For $I$ a real interval.
% 
%     Let $P$ be a $\log$-concave function on $I$ and $a, b \in \RR$ with $\forall t  \in I, at + b \geq 0$. Then $f: t \mapsto (at + b)P(t)$ is $\log$-concave on $I$.
% \end{lemma}
% \begin{proof}
%     Let $t \in I$.
% 
%     As $at + b \geq 0$, we have that $f(t) \geq 0$. We can therefore consider its logarithm (with that $\log(0) = -\infty$).
% 
%     We have that:
%     \begin{align}
%         f'(t) &= aP(t) + (at + b)P'(t) \\
%         f''(t) &= 2 aP'(t) + (at + b)P''(t) \\
%         f'(t)^2 &= a^2P(t)^2 + 2a(at + b)P(t)P'(t) + (at + b)^2P'(t)^2 \\
%         f(t)f''(t) &= 2a(at + b) P(t)P'(t) + (at + b)^2P(t)P''(t)
%     \end{align}
% 
%     Hence:
%     \[f'(t)^2 - f(t) f''(t) = a^2 P(t)^2 + (at + b)^2 \left[ P'(t)^2 - P(t)P''(t) \right] \]
% 
%     As $P$ is $\log$-concave on $I$, using the lemma~\ref{lemma:concavity_log_composed_functions} we have that $P'(t)^2 - P(t)P''(t) > 0$.
%     
%     As all the terms are $\geq 0$ we have that $\forall t \in I, f'(t)^2 - f(t) f''(t) \geq 0$ and using the lemma~\ref{lemma:concavity_log_composed_functions} we have that $f$ is $\log$-concave on $I$.
% \end{proof}
% 
% \begin{lemma}[Log concavity of the BOS model]
%     \label{lemma:log_concavity_bos_model}
%     $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m},, \forall e \subset \bbrack{1, m}$:
%     \[ \pi \mapsto \Pr(x | x \in e, e, \mu, \pi) \]
%     is $\log$-concave on $[0, 1]$.
% \end{lemma}
% \begin{proof}
%     Let $m \in \NN^*$, $x \in \bbrack{1, m}$ and $\mu \in \bbrack{1, m}$.
%     We proceed by induction on $\card{e}$:
% 
%     \[ P_n : \forall e \subset \bbrack{1, m}, |e| \leq n \Rightarrow \pi \mapsto \Pr(x | x \in e, e, \mu, \pi) \text{ is } \log\text{-concave on } [0, 1] \]
% 
%     \begin{itemize}
%         \item Initialization: $\card{e} = 1$:
%         \[\pi \mapsto \Pr(x | x \in e, e, \mu, \pi) = \indickronecker{e = \set{x}} \] which is $\log$-concave on $[0, 1]$.
%         Thus $P_1$ holds.
% 
%         \item Induction: Suppose $P_n$, the result holds for all $f \subset \bbrack{1, m}$ of size less or equal than $n$ and let us prove it for $n+1$.
%         
%         Let $e \subset \bbrack{1, m}$ such that $\card{e} = n + 1$.
% 
%         Using the lemma~\ref{lemma:bos_polynomial}, we have:
%         \begin{align}
%             \Pr(x | x \in e, e, \mu, \pi) 
%             &= \sum_{f \subset e ; x \in f} \Pr(x | x \in f, f, \mu, \pi) \Pr(f | x \in e, e, \mu, \pi)
%         \end{align}
% 
%         We have a sum of function. We now have to check that each function is $\log$-concave on $[0, 1]$. We will use the lemma~\ref{lemma:concavity_log_polynomial_times_affine}. 
%         
%         We first focus on $\Pr(f | x \in e, e, \mu, \pi)$.
%         Using the lemma~\ref{lemma:bos_transition}, we have that  $\Pr(f | x \in e, e, \mu, \pi)$ is an affine function of $\pi$ and a probability therefore it is of the form $a \pi + b$ with $\forall \pi \in [0, 1], a \pi + b \geq 0$. 
%         
%         Using $P_n$ we have that $\pi \mapsto \Pr(x | x \in f, f, \mu, \pi)$ is $\log$-concave. Hence, using the lemma~\ref{lemma:concavity_log_polynomial_times_affine}, we have that each $\pi \mapsto \Pr(x | x \in f, f, \mu, \pi) \Pr(f | x \in e, e, \mu, \pi)$ is $\log$-concave on $[0, 1]$.
%         
%         This gives us that $\Pr(x | x \in e, e, \mu, \pi)$ is a sum of $\log$-concave functions and is therefore $\log$-concave on $[0, 1]$.
% 
%         This is true for any $e$ of size $n + 1$ and therefore $P_{n+1}$ holds.
%         
%     \end{itemize}
% 
% \end{proof}
% 

\begin{conjecture}[Log concavity of the BOS model]
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}$:
    \[\pi \mapsto \Pr(x | \mu, \pi) \] 
    is $\log$-concave on $[0, 1]$.
\end{conjecture}
% \begin{proof}
%     We just have to apply the lemma~\ref{lemma:log_concavity_bos_model} to the case where $e = \bbrack{1, m}$.
% \end{proof}

Unfortunately, we have not yet been able to prove this theorem. However, we have successfully verified with high confidence that this property holds for $m \leq 94$. Our methodological approach for this empirical result involves checking the negativity of the second derivative of the log-likelihood. To do so we compute the second derivative and consider only the numerator since the denominator is always positive. Since the numerator is a polynomial, we can apply the following tests to verify its negativity on $[0,1]$:
\begin{itemize}
    \item If all the coefficients of the polynomial are negative, then the polynomial is negative on $[0,1]$. 
    \item Otherwise, we check if Lemma \ref{lemma:polynomial_negativity} applies.
    \item If it does not, we perform an efficient verification of the negativity of the polynomial by evaluating it on multiple points in $[0,1]$ as described below.
\end{itemize}
If any of these tests is successful, we can conclude that the second derivative is negative on $[0,1]$ and therefore the log-likelihood is concave on $[0,1]$.

\begin{lemma}
    \label{lemma:polynomial_negativity}
    Let $P$ be a polynomial function of degree $n$ on $[0, 1]$. Let $(a_i)_{i=0}^{n}$ be the coefficients of $P$ such that $P(x) = \sum_{i=0}^{n} a_i x^i$. Let $(p_i)_{i=0}^n$ be the positive part of $(a_i)_{i=0}^n$, i.e. $\forall i, p_i = \max(0,a_i)$. In that case, if $\sum_{i=1}^{n} p_i \leq -a_0$, then $P$ is negative on $[0, 1]$.
\end{lemma}


\begin{proof}
    Let $x \in [0, 1]$, $P(x) = \sum_{i=0}^{n} a_i x^i$. Define for all $i$, $p_i = \max(0, a_i)$ and $n_i = \min(0, a_i)$.

    We have that:
    \begin{align}
        \sum_{i=1}^{n} p_i \leq -a_0 &\implies \sum_{i=1}^{n} p_i x_i \leq -a_0 \qquad \text{since $x\in [0, 1]$} \\
        &\implies \sum_{i=1}^{n} p_i x_i + \sum_{i=1}^{n} n_i x_i \leq -a_0 \\
        &\iff \sum_{i=1}^{n} a_i x_i \leq -a_0 \\
        &\iff P(x) \leq 0
    \end{align}
\end{proof}

\paragraph*{Efficient verification of the negativity on $[0,1]$ of a polynomial} One straightforward approach is to evaluate the polynomial at a large number of points in the interval \([0,1]\) and check if it is always negative. However, this method lacks firm guarantees on the negativity of the polynomial and requires increasing computational resources as the number of points increases.

To address this, we propose a more efficient method for verifying the negativity of a polynomial on \([0,1]\) with confidence limited only by numerical errors of the machine.

Suppose we know an upper bound \(M\) on the derivative of the function on \([0,1]\). Then, for any \(x \in [0,1]\) and \(h \in \mathbb{R}_+\) such that \(x+h\leq 1\), we have \(P(x+h) \leq P(x) + hM\). Therefore, if \(P(x) \leq 0\) and \(h \leq -\frac{P(x)}{M}\), then \(P(x+h) \leq 0\).

This result allows us to devise an adaptive algorithm for verifying the negativity of a polynomial on \([0,1]\) by sampling the polynomial at intervals of varying lengths. The upper bound \(M\) can be estimated by computing the sum of the positive coefficients of degree \(i \geq 1\) of the derivative of the polynomial and the coefficient of degree \(0\) (regardless of sign). The algorithm is outlined in Algorithm~\ref{algo:check_polynomial_negativity}.

\begin{algorithm}[H]
    \caption{Check polynomial negativity}
    \begin{algorithmic}[1]
    \Require $P$ a polynomial function represented by its coefficients $(a_i)_{i=0}^n$ 
        
    \State Let $M=\text{ComputeDerivativeUpperBound}(P')$
    \State Let $x=0$
    \State Let $y=P(x)$
    \While{$x<1$ and $y\leq 0$}
        \State $y=P(x)$
        \State $x = x + \frac{-y}{M}$
    \EndWhile
    \State \Return $y\leq 0$
\end{algorithmic}

\label{algo:check_polynomial_negativity}
\end{algorithm}


\subsection{Efficient computation of the likelihood}

\subsubsection{Mathematical proofs}

\begin{lemma}[Symetries the likelihood]
    \label{lemma:symetries_bos}
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}, \pi \in [0, 1], \forall e = \bbrack{l, u} \subset \bbrack{1, m}$:
    
    \[ \Pr(x | x \in \bbrack{l, u}, e = \bbrack{l, u}, \mu, \pi) = \Pr(x - l | x - l \in \bbrack{0, u - l}, e= \bbrack{0, u - l}, \max(0, \mu - l), \pi)\]
\end{lemma}
\begin{proof}
    It is a simple translation of the categories. The only tricks is to notice that if $\mu$ is stricly less than the interval or equal to the lower bound, it will not affect the correctness of any sub-interval.
\end{proof}

\begin{definition}
    As justified by the lemma~\ref{lemma:symetries_bos}, we can define the following notation:
    \[ \bosl{x}{\mu}{h} := \Pr(x | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi) \]
\end{definition}

\begin{thm}[Computing the likelihood]
    \label{thm:computing_likelihood_bos}
    $\forall m \in \NN^*, \forall x \in \bbrack{1, m}, \forall \mu \in \bbrack{1, m}, \forall \pi \in [0, 1]$:

\begin{equation}
    \begin{aligned}
        \bosl{x}{\mu}{h}
        &=\frac{1}{h} \sum_{y = 0}^{x - 1} \bosl{x}{\mu}{y} \left[ \left( \indickronecker{\mu < y} - \frac{y}{h} \right) \pi + \frac{y}{h} \right] \\
        &+\frac{1}{h} \ \qquad \left[ \left( \indickronecker{\mu = x \lor (x = 0 \land \mu \leq x) \lor (x = h - 1 \land \mu \geq x)} - 1 \right) \pi +  \frac{1}{h} \right] \\
        &+\frac{1}{h} \sum_{y = x + 1}^{h - 1}\bosl{x - y}{\max(0, \mu - y)}{h - y}    \left[ \left( \indickronecker{\mu > y} - \frac{h - y - 1}{h} \right) \pi + \frac{h - y - 1}{h} \right]
    \end{aligned} \\
\end{equation}
\end{thm}

\begin{proof}

First we marginalize over the breakpoint $y$:
\begin{align}
    \bosl{x}{\mu}{h}
    &= \Pr(x | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi) \\
    &
    \begin{aligned}
        &= \sum_{y = 0}^{h - 1} \Pr(x, f=e^{-, y} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)\\
        &+ \sum_{y = 0}^{h - 1} \Pr(x, f=e^{=, y} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)\\
        &+ \sum_{y = 0}^{h - 1} \Pr(x, f=e^{+, y} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)
    \end{aligned} \\
\end{align}

Then we use the Bayes rule ($P(A, B) = P(A | B) P(B)$) to get likelihoods of $x$:

\begin{align}
    &
    \begin{aligned}
        &=\sum_{y = 0}^{h - 1} \Pr(x | x \in \bbrack{0, h - 1}, f = e^{-, y}, \mu, \pi) \Pr(e^{-, y} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)\\
        &+ \sum_{y = 0}^{h - 1} \Pr(x | x \in \bbrack{0, h - 1}, f = e^{=, y}, \mu, \pi) \Pr(e^{=, y} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)\\
        &+ \sum_{y = 0}^{h - 1} \Pr(x | x \in \bbrack{0, h - 1}, f = e^{+, y}, \mu, \pi) \Pr(e^{+, y} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)
    \end{aligned} \\
\end{align}

Then we can get rid of the case where $x \notin f$ as it is $0$: 
\begin{align}
    &
    \begin{aligned}
        &=\sum_{y = 0}^{x - 1} \Pr(x | x \in \bbrack{0, y - 1}, f = \bbrack{0, y - 1} , \mu, \pi) \Pr(\bbrack{0, y - 1} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)\\
        &+\ \qquad \Pr(x | x \in \set{x}, f = \set{x}, \mu, \pi) \Pr(\set{x} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)\\
        &+ \sum_{y = x + 1}^{h - 1} \Pr(x | x \in \bbrack{y + 1, h - 1}, f = \bbrack{y + 1, h - 1}, \mu, \pi) \\
        &\qquad \Pr(\bbrack{y + 1, h - 1} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)
    \end{aligned} \\
\end{align}

We can apply the lemma~\ref{lemma:symetries_bos} to the third term:
\begin{align}
    &
    \begin{aligned}
        &=\sum_{y = x + 1}^{h - 1} \Pr(x | x \in \bbrack{0, y - 1}, f = \bbrack{0, y - 1} , \mu, \pi) \Pr(\bbrack{0, y- 1} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)\\
        &+\ \qquad \Pr(x | x \in \set{x}, f = \set{x}, \mu, \pi) \Pr(\set{x} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)\\
        &+ \sum_{y =0}^{x - 1} \Pr(x - y - 1 | x - y - 1\in \bbrack{0, h - 1 - y - 1}, f = \bbrack{0, h - 1 - y - 1}, \max(0, \mu - y - 1), \pi) \\
        &\qquad \Pr(\bbrack{y + 1, h - 1} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)
    \end{aligned} \\
    &
    \begin{aligned}
        &=\sum_{y = x + 1}^{h - 1} \bosl{x}{\mu}{y - 1} \Pr(\bbrack{0, y-1} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)\\
        &+\ \qquad \bosl{x}{\mu}{1} \Pr(\set{x} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)\\
        &+ \sum_{y = 0}^{x - 1}\bosl{x - y - 1}{\max(0, \mu - y - 1)}{h - y - 1} \Pr(\bbrack{y + 1, h - 1} | x \in \bbrack{0, h - 1}, e = \bbrack{0, h - 1}, \mu, \pi)
    \end{aligned} \\
\end{align}

First we have $\bosl{x}{\mu}{1} = 1$.
Moreover, we can now use the lemma~\ref{lemma:bos_transition} to get replace the transition probabilities. In our case as we already selected the only possible interval for each breakpoint we have the only term of the sum where $\indickronecker{x \in \nextes{e}{y}} = 1$ and the sum is reduced to a single term:

\begin{align}
    \begin{aligned}
        &=\sum_{y = x + 1}^{h - 1} \bosl{x}{\mu}{y - 1} \frac{1}{h} \left[ \left( \correct{\mu}{\bbrack{0, h - 1}}{y - 1}{\bbrack{0, y - 1}} - \frac{y}{h} \right) \pi + \frac{y}{h} \right] \\
        &+\ \qquad \frac{1}{h} \left[ \left(\correct{\mu}{\bbrack{0, h - 1}}{x}{\set{x}} - \frac{1}{h} \right) \pi + \frac{1}{h} \right] \\
        &+ \sum_{y = 0}^{x - 1}\bosl{x - y}{\max(0, \mu - y)}{h - y} \frac{1}{h}  \\
        &\qquad \left[ \left( \correct{\mu - y - 1}{\bbrack{0, h - 1 - y - 1}}{h - y - 1}{\bbrack{y, h - 1}} - \frac{h - y - 1}{h} \right) \pi + \frac{h - y - 1}{h} \right]
    \end{aligned} \\ 
\end{align}

We can then replace $\correct{\mu}{\bbrack{0, h - 1}}{\bullet}{\bullet}$ by a logical expression. (We must take into account the special case of the first and last breakpoint):

\begin{align}
    \begin{aligned}
        &=\frac{1}{h} \sum_{y = x + 1}^{h - 1} \bosl{x}{\mu}{y} \left[ \left( \indickronecker{\mu < y} - \frac{y}{h} \right) \pi + \frac{y}{h} \right] \\
        &+\frac{1}{h} \ \qquad \left[ \left( \indickronecker{\mu = x \lor (x = 0 \land \mu \leq x) \lor (x = h - 1 \land \mu \geq x)} - 1 \right) \pi +  \frac{1}{h} \right] \\
        &+\frac{1}{h} \sum_{y = 0}^{x - 1}\bosl{x - y}{\max(0, \mu - y)}{h - y}    \left[ \left( \indickronecker{\mu > y} - \frac{h - y - 1}{h} \right) \pi + \frac{h - y - 1}{h} \right]
    \end{aligned} \\
\end{align}
\end{proof}


\subsubsection{Algorithm}

In the algorithm we consider that $u$ contains polynomials and that $\times_P$ and $+_P$ are respectively the multiplication and addition of polynomials.

on the line 16, we use the a $\min$ to avoid computing the polynomial for $\mu$ outside the considered range. As if $\mu$ is greater than the interval or equal to the upper bound, the correctness of any sub-interval is not different.

\begin{algorithm}[H]
    \caption{BOS polynomial computation}
    \begin{algorithmic}[1]
    \Require $m \in \NN$
    \Ensure $u[m -1, \mu - 1, x - 1] = \Pr(x | \mu, \pi)$ where we consider it as a polynomial equality  
        
    \State $u \leftarrow \text{array of size } m \times m \times m$ initialised to $0$
    \State $u[0] \leftarrow 1$
    \For{$h \in \bbrack{2, m}$}
        \For{$\mu \in \bbrack{0, h - 1}$}
            \For{$x \in \bbrack{0, h - 1}$}
                \State $s = u[h - 1, \mu, x]$
                \For{$y \in \bbrack{0, x - 1}$}
                    \State $p \leftarrow u[h - y - 2, \max(0, \mu - y - 1), x - y - 1]$
                    \State $a \leftarrow \frac{h - y - 1}{h}$
                    \State $s \leftarrow s +_P p \times_P \left[ \left( \indickronecker{\mu > y} - a \right) \pi +_P a \right]$
                \EndFor
                \State $c \leftarrow \indickronecker{\mu = x \lor (x = 0 \land \mu \leq x) \lor (x = h - 1 \land \mu \geq x)}$
                \State $a \leftarrow \frac{1}{h}$
                \State $s \leftarrow s +_P (c - a) \pi +_P a$
                \For{$y \in \bbrack{x + 1, h - 1}$}
                    \State $p \leftarrow u[y - 1, \min(\mu, y - 1), x]$
                    \State $a \leftarrow \frac{y}{h}$
                    \State $s \leftarrow s +_P p \times_P \left[ \left( \indickronecker{\mu < y} - a \right) \pi +_P  a \right]$
                \EndFor
                \State $u[h, \mu, x] = s / h$
            \EndFor
        \EndFor
    \EndFor    
\end{algorithmic}
\end{algorithm}


% \begin{align}
%     \Pr(x | x \in e_j, \mu, \pi) 
%     &= \sum_{e_{j+1} \subset e_j} \Pr(x, e_{j+1} | x \in e_j, \mu, \pi) \\
%     &= \sum_{e_{j+1} \subset e_j} \Pr(x | e_{j+1}, x \in e_j, \mu, \pi) \Pr(e_{j+1} | e_j, \mu, \pi) \\
%     &= \sum_{e_{j+1} \subset e_j ; x\in e_{j+1}} \Pr(x | x \in e_{j+1}, \mu, \pi) \Pr(e_{j+1} | e_j, \mu, \pi)
% \end{align}
% 
% We now suppose $e_j = \bbrack{l, h - 1}$:
% 
% \begin{align}
%     &\Pr(x | x \in \bbrack{l, h-1}, \mu, \pi) =\\
%     &\begin{aligned}
%         &\sum_{y=x + 1}^{h-1} \Pr(\bbrack{l, y - 1} | \bbrack{l, h-1}, \mu, \pi) \Pr(x | x \in \bbrack{l, y - 1}, \mu, \pi) \\
%         &+ \Pr(\set{x} | \bbrack{l, h-1}, \mu, \pi) \Pr(x | x \in \set{x}, \mu, \pi) \\
%         &+ \sum_{y=l}^{x - 1} \Pr(\bbrack{y + 1, h-1} | \bbrack{l, h-1}, \mu, \pi) \Pr(x | x \in \bbrack{y + 1, h-1}, \mu, \pi)
%     \end{aligned} \\
%     &= \begin{aligned}
%         &\frac{1}{h - l} \sum_{y=x + 1}^{h-1} \left[ \pi \indickronecker{\mu < y} + (1-\pi) \frac{y - l}{h - l} \right] \Pr(x | x \in \bbrack{l, y - 1}, \mu, \pi) \\
%         &+\frac{1}{h - l} \left[ \pi \indickronecker{\mu = x \lor (x = l \land \mu \leq x) \lor (x = h - 1 \land \mu \geq x)} + (1 - \pi) \frac{1}{h-l} \right] \\
%         &\Pr(x | x \in \set{x}, \mu, \pi) \\
%         &+\frac{1}{h - l} \sum_{y=l}^{x - 1} \left[ \pi \indickronecker{\mu > y} + (1-\pi) \frac{h - y - 1}{h - l} \right] \Pr(x | x \in \bbrack{y + 1, h-1}, \mu, \pi)
%     \end{aligned}
% \end{align}
% 
% As $\Pr(x | x \in \set{x}, \mu, \pi) = 1$ this allows to compute the probability of $x$ being in the interval $\bbrack{l, h-1}$ recursively.
% 
% As:
% \begin{equation}
%     \Pr(x | x \in \bbrack{l, y - 1}, \mu, \pi) = \Pr(x - l | x - l \in \bbrack{0, y - l - 1}, \max(0, \mu - l), \pi)
% \end{equation}
% 
% We can rewrite the previous equation as:
% 
% \begin{align}
%     h\Pr(x | x \in \bbrack{0, h - 1})
%     &= \sum_{y = x + 1}^{h - 1} \left[ \pi \indickronecker{\mu < y} + (1-\pi) \frac{y}{h} \right] \Pr(x | x \in \bbrack{0, y - 1}, \mu, \pi) \\
%     &+ \pi \indickronecker{\mu = x \lor (x = 0 \land \mu \leq x) \lor (x = h - 1 \land \mu \geq x)} + (1 - \pi) \frac{1}{h} \\
%     &+ \sum_{y = 0}^{x - 1} \left[ \pi \indickronecker{\mu > y} + (1-\pi) \frac{h - y - 1}{h} \right] \Pr(x - y - 1 | x - y - 1 \in \bbrack{0, h - y - 2}, \max(0, \mu - y - 1), \pi)
% \end{align}
% 
% 

