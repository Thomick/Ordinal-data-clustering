\subsection{Globally Ordered Data model}


% \tm {Laisser présentation du modèle, graphical model, estimation des paramètres. Tous les lemmes et théorèmes vont en annexe.}

The authors of~\cite{biernacki2016model} motivated the use of binary search with the following sentence: "In order to minimize the number of potentially wrong comparisons, it
is necessary to minimize the number of comparisons performed during the search process.". However, we believe that minimizing the number of incorrect comparisons may not be an adequate intuition, and it is more crucial to minimize the probability of making a wrong guess. Motivated by this perspective, we have developed an alternative model where the data is compared with each category with some noise. We will refer to this model as the Globally Ordered Data (GOD) model.

We still consider a search for the parameter $\mu$ among the ordered categories. However, instead of conducting a binary search, we compare each category to the parameter $\mu$ and with probability $\pi$ we get the correct answer. After making all these comparisons, we then select the category that corresponds to the minimum number of comparison error. This approach display similar properties to the BOS model such as a unique mode, probability distribution decrease on either side of the mode, and the flexibility to accommodate uniform or Dirac distribution.

\subsubsection{Probabilistic model}

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
     \node[obs]                               (xj) {$x_i$};
     \node[latent, left=1.4cm of xj]               (cj) {$C[j]$};
     \node[latent, left=of cj]               (zj) {$Z_j$};
     \node[const, left=of zj]    (pi) {$\pi$};
     \node[const, above=of cj]  (mu) {$\mu$};
    
     \edge{zj}{cj};
     \edge{cj}{xj};
     \edge {mu} {cj};
     \edge {pi} {zj};
    
     \plate[inner sep=.55cm] {}{(xj)(cj)(zj)}{$i=1, \ldots, n$};
     \plate[inner sep=.25cm] {}{(cj)(zj)}{$j=1, \ldots, m - 1$};
    \end{tikzpicture}
    \caption{Graphical model of the GOD model.}
    \label{fig:god_graphical_model}
\end{figure}
    
The GOD model for $m$ categories is characterized by two parameters $\mu \in \bbrack{1, m}, \pi \in ]\frac{1}{2}, 1]$. The observed data is only the selected category $x$. The latent variables are the vector $Z = (Z_1,...,Z_{m-1}) \in \set{0, 1}^{m - 1}$ and $C \in \set{0, 1}^{m-1}$.
$(Z_j)_{j\in\bbrack{1,m-1}}$ is a vector of independent Bernoulli variables, with parameter $\pi$. For $j \in \bbrack{1, m-1}$, $Z_j$ indicates whether the comparison with the category $j$ is correct $(Z_j=1)$ or not $(Z_j=0)$. $C$ is the vector containing the $m$ results of the comparisons depending on both $Z$ and the parameter $\mu$. It is defined as follows:
\[
\forall j \in \bbrack{1, m-1}, C[j] = \begin{cases}
    (\mu < j) & \text{if}\  Z_j = 1\\
    (\mu \not< j) & \text{if}\  Z_j = 0
\end{cases} \]

The GOD model will generate $x \in \bbrack{1, m}$ such that $x \sim \mathcal{U}(\argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1)$. We can interpret this as a probability maximization as stated in Theorem~\ref{thm:projection}. The graphical model associated with this probabilistic model is depicted on Figure \ref{fig:god_graphical_model}.


\begin{definition}[Heaviside vector]
    For $k \in \bbrack{1, m}$, we define:
    \[E_k := (1)^{k-1} (0)^{m - k} = (\underset{k-1}{\underbrace{1, \dots, 1}}, \underset{m - k}{\underbrace{0, \dots, 0}} ). \]
\end{definition}


\begin{thm}
    \label{thm:projection}
    If we suppose that the prior distribution of $\mu$ is uniform over $\bbrack{1, m}$ and $\pi > \frac{1}{2}$, then \(\forall c \in \set{0, 1}^{m-1}\),
    \[\argmax_{k \in \bbrack{1, m}} \Pr(\mu = k | C = c) = \argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1.\]
\end{thm}

The proof can be found in appendix~\ref{thm:projection_appendix}.

\subsubsection{Parameter estimation}

We want to estimate $\pi$ and $\mu$ from a sample $(1, \dots, m)$ with weights $W \in \RR_{+}^m$ generated by the GOD model. We aim at maximizing the likelihood of the sample : $\Pr(W | \pi, \mu)$. We proceed as explained in section~\ref{sec:univariate_generic_estimation}.

\paragraph{Likelihood evaluation}

\begin{definition}
    We define for $x \in \bbrack{1, m}$,:
\[ \mathcal{C}_x := \set{c \in \set{0, 1}^{m-1} | x \in \argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1 }\]
\end{definition}

\begin{definition}
    We define for $x \in \bbrack{1, m}, \mu \in \bbrack{1, m}, d \in \bbrack{0, m-1}$:
    \[ u(\mu, x, d) := \sum_{c \in \mathcal{C}_x / \norm{c - E_{\mu}}_1 = d}  \card{\argmin_{k \in \bbrack{1, m}} \norm{c - E_k}_1}^{-1}\]
\end{definition}

Using these definitions we have the expression of the likelihood of a single observation.

\begin{thm}[Observation likelihood]
    \label{thm:p_x_knowing_pi_mu}
    \[\Pr(x | \pi, \mu) = \pi^{m-1} \sum_{d = 0}^{m-1} \left(\frac{1 - \pi}{\pi}\right)^d u(x, \mu, d)\]
\end{thm}
\begin{proof}
    See appendix~\ref{thm:p_x_knowing_pi_mu}.
\end{proof}

We can notice that once $u$ is computed, the likelihood of a single observation can be computed in $\Theta(m)$ operations and the weighted likelihood can be computed in $\Theta(m^2)$ operations. 

\paragraph{Computing the coefficients $u$}

The computation of the coefficients $u$ can be done in $\mathcal O(m^2 2^m)$ time. We believe that it might be possible to compute it in polynomial time, but we did not have time to find how. Although still costly, it only needs to be computed once for a given $m$ and can be stored in $\mathcal O(m^3)$ space.

\paragraph{Log-concavity}

\begin{thm}
    \label{thm:log_likelihood_concave}
    $\forall \mu \in \bbrack{1, m},\forall x \in \bbrack{1, m}$, 
    \[ \pi \mapsto \Pr(x|\pi, \mu) \]
    is stricly $\log$-concave on $]\frac{1}{2}, 1[$ and $\log$-concave on $[\frac{1}{2}, 1]$.
\end{thm}
\begin{proof}
    See appendix~\ref{thm:log_likelihood_concave_appendix}.
\end{proof}

As explained in section~\ref{sec:univariate_generic_estimation}, we directly have that $\forall \mu \in \bbrack{1, m}, \pi \mapsto L_W(\pi, \mu)$ is concave. Hence we can use a ternary search algorithm to estimate $\pi$ for a given $\mu$.

\paragraph{Conclusion}

As presented in section~\ref{sec:univariate_generic_estimation}, we can estimate $\mu, \pi$ in $\mathcal O(m^3 \log \frac{1}{\epsilon})$ operations once the coefficients $u$ are computed. The coefficients $u$ can be computed in $\mathcal O(m^2 2^m)$ operations and stored in $\mathcal O(m^3)$ space. This may seem costly, but $m$ is usually small. (In comparison in~\cite{biernacki2016model} they assume that $m \leq 7$)
