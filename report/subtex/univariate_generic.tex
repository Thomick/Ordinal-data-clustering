\subsection{Univariate model}
\label{sec:univariate}

We now wan't a random process to model univariate ordinal data among a finite numbers of categories.
As we suppose that we only care for the order of the categories we can can without loss of generality consider our categories as $\bbrack{1, m}$ (when there is $m$ categories). Therefore if we have $\theta$ the parameters of our model, a model is gives $\forall i \in \bbrack{1, m}, P(X = i | \theta)$ if $X$ was generated from our random process. 

As we should represent data have having a common source we can suppose that there is an underlying true category $\mu \in \bbrack{1, m}$ and put it as a parameter. In addition it is natural to add a precision parameter $\pi$. This is the case for the BOS model and the GOD model. 

In the following sections we present how to estimate those parameters for a generic law, then we present the BOS model and how to apply this estimation technique then we do the same with the GOD model.


\subsubsection{Notations and goal}

Let suppose that we have a set of $n$ independant observations $X = (x_i)_{i \in [n]}$, wehre $x_i \in \bbrack{1, m}$ follow a distribution $P$ with parameters $\mu, \pi$ with $\mu \in \bbrack{1, m}$ and $\pi \in [[a, b]]$. We want to estimate $\mu$ and $\pi$. We choose the estimate that maximize the likelihood of the data.
$$(\mu, \pi) = \argmax_{(\mu, \pi) \in \bbrack{1, m} \times [a, b]} P(X | \mu, \pi)$$

As the data are independant, we have:
$$P(X | \mu, \pi) = \prod_{i=1}^n P(x_i | \mu, \pi)$$

As the number of possible values for $x$ is finite, we can group the data by values and count the number of occurences of each value. Let $n_i$ be the number of occurences of $i$ in the data. We have:
$$P(X | \mu, \pi) = \prod_{i=1}^m P(i | \mu, \pi)^{n_i}$$

In the AECM algorithm, each data point has a weight $w_i$. As previously, we can suppose without loss of generality that we have only one observation of each value with a specific weight (we can always group the data by values and sum the weights of the observations). With the weights $W \in \mathbb{R}_+^n$ where $w_i$ is the weight of the value $i$, we can write the weighted likelihood as:

$$P(W | \mu, \pi) = \prod_{i=1}^m P(i | \mu, \pi)^{w_i}$$

We can also write the weighted $\log$-likelihood as:

$$L_W(\mu, \pi) := \log P(W | \mu, \pi) = \sum_{i=1}^m w_i \log P(i | \mu, \pi)$$


\subsubsection{Optimization}
\label{sec:univariate_generic_estimation}

To estimate $\mu$ and $\pi$, the idea proposed for the BOS-model in REF is to use the Expectaion-Maximization algorithm. However they note that it is easier to first estimate $\pi$ for every possible value of $\mu$ and then to estimate $\mu$ using the estimated $\pi$. In forulas, we have:


$$\hat{\pi_{\mu}} = \argmax_{\pi \in [[a, b]]} P(W | \mu, \pi)$$
$$\hat{\mu} = \argmax_{\mu \in [[1, m]]} \max_{\pi \in [[a, b]]} P(W | \mu, \pi) = \argmax_{\mu \in [[1, m]]} P(W | \mu, \hat{\pi_{\mu}})$$

Once we have the estimates $\hat{\pi_{\mu}}$ for every possible value of $\mu$, it is easy to estimate $\mu$ by choosing the value of $\mu$ that maximize the weighted likelihood as it requires only to compute the likelihood for every possible value of $\mu$ and to choose the maximum.

Estimating $\pi$ for a given $\mu$ is a one-dimensional optimization problem. We can use the EM algorithm to solve it but it may be possible to use a direct optimization algorithm. For example if the function $\pi \mapsto L_W(\mu, \pi)$ is stricly concave (or constant), we can the ternary search algorithm (or trisection algorithm) to find the maximum.

\paragraph{Concavity of the weighted likelihood}

Suppose we have:

$$\forall x \in [[1, m]], \pi \mapsto P(x | \mu, \pi) \text{ is strcictly $\log$-concave}$$

Then we have, $\pi \mapsto L_W(\mu, \pi)$ is stricly concave as positive linear combination of concave functions are concave.

\paragraph{Ternary search algorithm}


\tr{TODO: ref} 
%(see https://en.wikipedia.org/wiki/Ternary_search)

The ternary search algorithm can be used to find the argmax of a unimodal  concave function on a given interval with a precision of $\epsilon$ in $\Theta(\log \frac{b - a}{\epsilon})$ evaluations of the function. More precisely in our case ($b - a \leq 1$) it will require approximately $100$ evaluations of the function for a precision of $10^{-10}$.
We will use this algorithm to estimate $\pi$ for a given $\mu$.



\paragraph{Evaluating the likelihood}

To run the ternary search algorithm, we need to be able to evaluate the log-likelihood for a given value of $\pi$ efficiently.
The log-likelihood is the sum of $m$ log of the likelihoods of individual values.
Hence the complexity will be $\Theta(m C_E(m))$ where $C_E(m)$ is the complexity of computing the likelihood for a single value of $x$.

In the case of the BOS model, we can notice that $\forall x \in [[1, m]], \pi \mapsto P(x | \mu, \pi)$ is polynomial of degree $\Theta(m)$ with coefficients that depend on $m$, $i$ and $\mu$. This is alsmost alos the case for the GOD mdoel in the sense that the following reasonning holds. As the function is polynomial, we can precompute these coefficients and then evaluate the likelihood of one value in $\Theta(m)$ operations. This gives a complexity of $\Theta(m^2)$ to evaluate the total likelihood of $W$ for a given $\mu$ and $\pi$. 

An important point is we excluded the cost of the precomputations of the coefficients. This cost is not necessary negligible but countrary to the evaluation of the likelihood, it is only done once for one $m$ and can be then used for all iterations of the AECM algorithm and the ternary search algorithm.


\paragraph{Complexity}

We note $n$ the numbre of observations, $m$ the number of possible values for $x$ and $\epsilon > 0$ the precision of the estimate of $\pi$.

We can first group the data by values and sum the weights of the observations. This can be done in $\Theta(n)$ operations.

Then to estimate $\pi$ for a given $\mu$, we can use the ternary search algorithm. The number of iterations of the algorithm is $\Theta(\log \frac{b - a}{\epsilon})$. For each iteration, we need to compute the likelihood for two values of $\pi$. If we note $C_E(m)$ the complexity of computing the likelihood for a given value of $\pi$, we have a complexity of $\Theta(\log \frac{b - a}{\epsilon} C_E(m))$.

Finally, to estimate $\mu$, we need to compute the likelihood for every possible value of $\mu$. This gives a total complexity of $\Theta(m C_E(m) \log \frac{b - a}{\epsilon} )$.

In our case we have $C_E(m) = \Theta(m^2)$ and $[a, b] \subset [0, 1]$ which gives a complexity of $\Theta(m^3 \log \frac{1}{\epsilon})$ (without taking into account the precomputations of the coefficients).
