\subsection{Univariate model}
\label{sec:univariate}

We now seek a random process to model univariate ordinal data among a finite number of categories. Assuming that we only concern ourselves with the order of the categories, we can, without loss of generality, consider our categories as $\bbrack{1, m}$ (when there are $m$ categories), as long as the model does not utilize the distance between the numbers associated with two categories. Therefore, for the parameters $\theta$ of our model, the model gives $\forall i \in \bbrack{1, m}, P(X = i | \theta)$ if $X$ was generated from our random process.

As the goal is to represent data having a common distribution, we can suppose that there is an underlying true category $\mu \in \bbrack{1, m}$ and treat it as a parameter. Additionally, it is natural to include a precision parameter $\pi$, which correspond to how much noise is involved in the selection of a category. This holds true for both the BOS model and the GOD model.

In the following sections, we demonstrate how to estimate those parameters for a generic law. Later, we introduce the BOS model and explain how to apply this estimation technique. Likewise, we present how these techniques can be adapted in the context of the GOD model.


\subsubsection{Notations and goal}

Let's suppose we have a set of $n$ independent observations $X = (x_i)_{i \in [n]}$, where $x_i \in \bbrack{1, m}$ follows a distribution $P$ with parameters $\mu, \pi$, where $\mu \in \bbrack{1, m}$ and $\pi \in [[a, b]]$. Our goal is to estimate $\mu$ and $\pi$ by selecting the estimates that maximize the likelihood of the data.
$$(\mu, \pi) = \argmax_{(\mu, \pi) \in \bbrack{1, m} \times [a, b]} P(X | \mu, \pi)$$

As the data points are independent, we have:
$$P(X | \mu, \pi) = \prod_{i=1}^n P(x_i | \mu, \pi)$$

Since the number of possible values for $x$ is finite, we can group the data by values and tally the occurrences of each value. Let $n_i$ represent the number of occurrences of $i$ in the data. We can write the likelihood as:
$$P(X | \mu, \pi) = \prod_{i=1}^m P(x_i | \mu, \pi)^{n_i}$$

In the AECM algorithm, each data point is assigned a weight $w_i$. As before, we can assume without loss of generality that we have only one observation of each value with a specific weight (we can always group the data by values and sum the weights of the observations). With the weights $W \in \mathbb{R}_+^n$, where $w_i$ is the weight of the value $i$, we can express the weighted likelihood as:


$$P(W | \mu, \pi) = \prod_{i=1}^m P(i | \mu, \pi)^{w_i}$$

We can also write the weighted $\log$-likelihood as:

$$L_W(\mu, \pi) := \log P(W | \mu, \pi) = \sum_{i=1}^m w_i \log P(i | \mu, \pi)$$


\subsubsection{Optimization}
\label{sec:univariate_generic_estimation}

To estimate $\mu$ and $\pi$, the approach suggested for the BOS model in \cite{biernacki2016model} is to employ the Expectation-Maximization algorithm. However, they highlight that it is simpler to initially estimate $\pi$ for every possible value of $\mu$ and then estimate $\mu$ using the obtained $\pi$. The corresponding formulas are


$$\hat{\pi}_{\mu} = \argmax_{\pi \in [a, b]} P(W | \mu, \pi)$$
$$\hat{\mu} = \argmax_{\mu \in \bbrack{1, m}} \max_{\pi \in [a, b]} P(W | \mu, \pi) = \argmax_{\mu \in \bbrack{1, m}} P(W | \mu, \hat{\pi}_{\mu}).$$

Estimating $\pi$ for a given $\mu$ is a one-dimensional optimization problem. We can utilize the EM algorithm for solving it, but it is also possible to employ a direct optimization algorithm, as we will demonstrate. For instance, if the function $\pi \mapsto L_W(\mu, \pi)$ is concave, we can use the ternary search algorithm to find the maximum.

\paragraph{Concavity of the weighted likelihood}

Suppose we have:

$$\forall x \in \bbrack{1, m}, \pi \mapsto P(x | \mu, \pi) \text{ is $\log$-concave}$$

Then, $\pi \mapsto L_W(\mu, \pi)$ is  concave since a positive linear combination of concave functions is concave.

\paragraph{Ternary search algorithm}


The ternary search algorithm\footnote{\url{https://en.wikipedia.org/wiki/Ternary_search}} can be employed to find the argmax of a unimodal or concave function within a given interval with a precision of $\epsilon$ in $\Theta(\log \frac{b - a}{\epsilon})$ function evaluations. More precisely, in our case ($b - a \leq 1$), it will require approximately $100$ evaluations of the function for a precision of $10^{-10}$. We will use this algorithm to estimate $\pi$ for a given $\mu$.



\paragraph{Evaluating the likelihood}

To execute the ternary search algorithm, we must efficiently evaluate the log-likelihood for a given $\pi$ value. The log-likelihood is the sum of the logarithms of individual values' likelihoods, Therefore, the complexity of this task is $\Theta(m C_E(m))$ where $C_E(m)$ represents the complexity of calculating the likelihood for a single value of $x$.

For the BOS model, we observe that $\forall x \in \bbrack{1, m}, \pi \mapsto P(x | \mu, \pi)$ is a polynomial of degree $\Theta(m)$ with coefficients depending on $m$, $i$, and $\mu$. This observation is almost applicable to the GOD model as well. Since the function is polynomial, we can precompute these coefficients and then evaluate the likelihood of one value in $\Theta(m)$ operations. This results in a complexity of $\Theta(m^2)$ for evaluating the total likelihood of $W$ for a given $\mu$ and $\pi$.

An essential consideration is that we've omitted the cost of precomputing the coefficients. While this cost is not necessarily negligible, unlike the likelihood evaluation, it occurs only once for a given $m$ and can subsequently be applied across all iterations of the AECM algorithm and the ternary search algorithm.


\paragraph{Complexity}

Let $n$ denote the number of observations, $m$ represent the number of possible values for $x$, and $\epsilon > 0$ indicate the precision of the estimate of $\pi$.

We can first group the data by values and sum the weights of the observations. This can be done in $\Theta(n)$ operations.

Next, to estimate $\pi$ for a given $\mu$, we can employ the ternary search algorithm. The number of iterations for the algorithm is $\Theta(\log \frac{b - a}{\epsilon})$. In each iteration, computing the likelihood for two $\pi$ values incurs a complexity of $\Theta(\log \frac{b - a}{\epsilon} C_E(m))$, where $C_E(m)$ denotes the complexity of computing the likelihood for a given $\pi$ value.

Finally, to estimate $\mu$, we must calculate the likelihood for every potential value of $\mu$. This results in a total complexity of $\Theta(m C_E(m) \log \frac{b - a}{\epsilon} )$.

In our specific scenario, where $C_E(m) = \Theta(m^2)$ and $[a, b] \subset [0, 1]$, this yields a complexity of $\Theta(m^3 \log \frac{1}{\epsilon})$ (excluding the precomputations of coefficients).
