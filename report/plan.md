# Plan du rapport:
## Intro:
- motivation
- état de l'art
- contribution par rapport au papier original
- plan

## Méthodes:
- description AECM
- description BOS
    - propriétés de BOS
    - description EM pour BOS
    - description trichotomy (principe, preuve, complexité...)
    - decription trichotomy pour BOS
- description de GOD
    - trichotomy pour GOD

## Expériences:
- comparaison temps de calcul EM-BOS, trichotomy-BOS, GOD
- comparaison des méthodes sur des données synthétiques
- comparaison des méthodes sur des données réelles


## Conclusion:

## Bibliographie

## Annexes

- détails des algorithmes
- détails des expériences




# Method 

We suppose that we aim to cluster multivariate data. Each dimension has data generated by a random process parametrized by parameters that are dependant of the cluster. To estimate the cluster it is possible to use the AECM algorithm introduce in~\cite{mengEMAlgorithmOld1997}. This algorithm is quite generic and only requires to be able to estimate the univariate parameters of a weighted set of data points. We present this algorithm in section~ref{sec:aecm}.

Similarly to~\cite{biernacki2016model} we focus on ordinal data. Therefore we suppose that each dimension follow a process that generates ordinal categories. 
In section~\ref{sec:univariate}, we present two random process to model ordinal data the binary ordinal search (BOS) model from~\cite{biernacki2016model} and the globally ordered data (GOD) model.

To apply the proposed methods the each dimension of the data should represent an ordinal categorie. These categories must respect the following properties:

\begin{itemize}
- \item The categories must be well-ordered (ordinal data): The categories are linearly ordered, and each non-empty subset contains the least element. This implies that any element can be compared to any other, and we can enumerate all the categories in increasing order.
- \item The set of categories is finite. This simplifies the previous assumption to the existence of a linear ordering. This assumption is necessary to ensure that the stochastic search terminates after a fixed number of steps, implying a finite number of possible runs of the search.
\end{itemize}


# AECM
\label{sec:aecm}


# Univariate model
\label{sec:univariate}

We now wan't a random process to model univariate ordinal data among a finite numbers of categories.
As we suppose that we only care for the order of the categories we can can without loss of generality consider our categories as $[[1, m]]$ (when there is $m$ categories). Therefore if we have $\theta$ the parameters of our model, a model is gives $\forall i \in [[1, m]], P(X = i | \theta)$ if $X$ was generated from our random process. 

As we should represent data have having a common source we can suppose that there is an underlying true category $\mu \in [[1, m]]$ and put it as a parameter. In addition it is natural to add a precision parameter $\pi$. This is the case for the BOS model and the GOD model. 

In the following sections we present how to estimate those parameters for a generic law, then we present the BOS model and how to apply this estimation technique then we do the same with the GOD model.


## Setting:

Let suppose that we have a set of $n$ independant observations $X = (x_i)_{i \in [n]}$, wehre $x_i \in [[1, m]]$ follow a distribution $P$ with parameters $\mu, \pi$ with $\mu \in [[1, m]]$ and $\pi \in [[a, b]]$. We want to estimate $\mu$ and $\pi$. We choose the estimate that maximize the likelihood of the data.
$$(\mu, \pi) = \argmax_{(\mu, \pi) \in [[1, m]] \times [[a, b]]} P(X | \mu, \pi)$$

As the data are independant, we have:
$$P(X | \mu, \pi) = \prod_{i=1}^n P(x_i | \mu, \pi)$$

As the number of possible values for $x$ is finite, we can group the data by values and count the number of occurences of each value. Let $n_i$ be the number of occurences of $i$ in the data. We have:
$$P(X | \mu, \pi) = \prod_{i=1}^m P(i | \mu, \pi)^{n_i}$$

In the AECM algorithm, each data point has a weight $w_i$. As previously, we can suppose without loss of generality that we have only one observation of each value with a specific weight (we can always group the data by values and sum the weights of the observations). With the weights $W \in \mathbb{R}_+^n$ where $w_i$ is the weight of the value $i$, we can write the weighted likelihood as:

$$P(W | \mu, \pi) = \prod_{i=1}^m P(i | \mu, \pi)^{w_i}$$

We can also write the weighted $\log$-likelihood as:

$$L_W(\mu, \pi) := \log P(W | \mu, \pi) = \sum_{i=1}^m w_i \log P(i | \mu, \pi)$$


## Optimization: 

To estimate $\mu$ and $\pi$, the idea proposed for the BOS-model in REF is to use the Expectaion-Maximization algorithm. However they note that it is easier to first estimate $\pi$ for every possible value of $\mu$ and then to estimate $\mu$ using the estimated $\pi$. In forulas, we have:


$$\hat{\pi_{\mu}} = \argmax_{\pi \in [[a, b]]} P(W | \mu, \pi)$$
$$\hat{\mu} = \argmax_{\mu \in [[1, m]]} \max_{\pi \in [[a, b]]} P(W | \mu, \pi) = \argmax_{\mu \in [[1, m]]} P(W | \mu, \hat{\pi_{\mu}})$$

Once we have the estimates $\hat{\pi_{\mu}}$ for every possible value of $\mu$, it is easy to estimate $\mu$ by choosing the value of $\mu$ that maximize the weighted likelihood as it requires only to compute the likelihood for every possible value of $\mu$ and to choose the maximum.

Estimating $\pi$ for a given $\mu$ is a one-dimensional optimization problem. We can use the EM algorithm to solve it but it may be possible to use a direct optimization algorithm. For example if the function $\pi \mapsto L_W(\mu, \pi)$ is stricly concave (or constant), we can the ternary search algorithm (or trisection algorithm) to find the maximum.

### Concavity of the weighted likelihood:

Suppose we have:

$$\forall x \in [[1, m]], \pi \mapsto P(x | \mu, \pi) \text{ is strcictly $\log$-concave}$$

Then we have, $\pi \mapsto L_W(\mu, \pi)$ is stricly concave as positive linear combination of concave functions are concave.

### Ternary search algorithm:

TODO (see https://en.wikipedia.org/wiki/Ternary_search)

### Complexity:

We note $n$ the numbre of observations, $m$ the number of possible values for $x$ and $\epsilon > 0$ the precision of the estimate of $\pi$.

We can first group the data by values and sum the weights of the observations. This can be done in $\Theta(n)$ operations.

Then to estimate $\pi$ for a given $\mu$, we can use the ternary search algorithm. The number of iterations of the algorithm is $\Theta(\log \frac{b - a}{\epsilon})$. For each iteration, we need to compute the likelihood for two values of $\pi$. If we note $C_E(m)$ the complexity of computing the likelihood for a given value of $\pi$, we have a complexity of $\Theta(\log \frac{b - a}{\epsilon} C_E(m))$.

Finally, to estimate $\mu$, we need to compute the likelihood for every possible value of $\mu$. This gives a total complexity of $\Theta(m C_E(m) \log \frac{b - a}{\epsilon} )$.

In our case we will have $C_E(m) = \Theta(m)$ and $[a, b] \subset [0, 1]$ which gives a complexity of $\Theta(m^2 \log \frac{1}{\epsilon})$.

# BOS Model

## Description

## Computation


# GOD Model

# Description

# Computation




Recycle this:

\paragraph{Mixture model} 
The BOS model can be extended to a mixture model by considering multiple BOS models with different parameters. In this case, the data is generated by first sampling a cluster $k$ from a multinomial distribution with parameter $\alpha$ and then sampling the data from the BOS model with parameters $\mu_k$ and $\pi_k$.

We can also consider a multivariate version of the BOS model in addition to the mixture case where we have a BOS model for each feature independently. Each dimension is then concatenated to get the multivariate dataset that follows a multivariate BOS distribution.

Therefore, for $p$ clusters, and $d$ features, the parameters of the model are $\alpha \in \mathbb{R}^p$, $\mu \in \mathbb{R}^{p\times d}$ and $\pi \in \mathbb{R}^{p\times d}$
Since the features are independent, the probability of observing a sample $x$ knowing it belongs to cluster $k$ is given by:
\begin{equation}
    \mathbb{P}(x | \alpha_k, \mu_{k}, \pi_k) = \prod_{j=1}^d \mathbb{P}(x_j | \alpha_k, \mu_{kj}, \pi_{kj})
\end{equation}
