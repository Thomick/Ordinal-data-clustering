\documentclass[a4paper,12pt]{article}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{adjustbox}
% \usepackage[margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{xcolor}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{bussproofs}
\usepackage{listings, lstautogobble}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{float}
\usepackage{appendix}
\usepackage{natbib}
\bibliographystyle{plainnat}
%\usepackage[french]{babel}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage[percent]{overpic}
\usepackage{todonotes}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usetikzlibrary{bayesnet}



\newcommand{\dd}{\text{d}}
\renewcommand{\vec}{\overrightarrow}
\renewcommand{\Im}{\mathfrak{Im}\ }
\newcommand{\Ker}{\text{Ker}\ }
\newcommand{\bijective}{%
  \hookrightarrow\mathrel{\mspace{-15mu}}\rightarrow
}
\newcommand{\surjective}{\twoheadrightarrow}
\newcommand{\injective}{\hookrightarrow}
\newcommand{\implication}{\Longrightarrow}
\newcommand{\reciprocal}{\Longleftarrow}
\newcommand{\equivalent}{\Longleftrightarrow}
\newcommand{\NN}{\ensuremath{\mathbb{N}}}
\newcommand{\RR}{\ensuremath{\mathbb{R}}}
\newcommand{\QQ}{\ensuremath{\mathbb{Q}}}
\newcommand{\ZZ}{\ensuremath{\mathbb{Z}}}
\newcommand{\CC}{\ensuremath{\mathbb{C}}}
\newcommand{\EE}{\ensuremath{\mathbb{E}}}
\newcommand{\PP}{\ensuremath{\mathbb{P}}}
\newcommand{\II}{\ensuremath{\mathds{1}}}
\newcommand{\cR}{\ensuremath{\mathcal{R}}}
\newcommand{\cY}{\ensuremath{\mathcal{Y}}}
\newcommand{\cZ}{\ensuremath{\mathcal{Z}}}
\newcommand{\cX}{\ensuremath{\mathcal{X}}}
\newcommand{\cN}{\ensuremath{\mathcal{N}}}
\newcommand{\cT}{\ensuremath{\mathcal{T}}}

\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\comb}[2]{\displaystyle{#2 \choose #1}}
\newcommand{\parts}{\mathcal{P}}
\newcommand{\permut}{\mathfrak{S}}
\newcommand{\id}{\text{Id}}
\newcommand{\indic}{\mathds{1}}
\newcommand{\indickronecker}[1]{\indic\left\{#1\right\}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\mat}[2]{\mathcal{M}_{#1}(#2)}

\newcommand{\set}[1]{\ensuremath{\left\{ #1 \right\}}}
\newcommand{\Tau}{\mathcal{T}}

\newcommand{\norm}[1]{\ensuremath{\left|\left|#1\right|\right|}}
\newcommand{\card}[1]{\ensuremath{\left|#1\right|}}

\DeclareMathOperator{\mut}{mut}
\DeclareMathOperator{\stab}{Stab}
\DeclareMathOperator{\sg}{sg}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\renewcommand{\time}{\textsc{Time}}
\newcommand{\len}{\textsc{Length}}
\newcommand{\call}[1]{\textsc{#1}}
\newcommand{\bbrack}[1]{\left\llbracket#1\right\rrbracket}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\DeclareMathOperator{\ucb}{UCB}
\DeclareMathOperator{\lcb}{LCB}

\newcommand{\tm}[1]{\todo[inline,color=orange!40]{{\textbf{TM:}~}#1}}
\newcommand{\TM}[1]{{\color{orange!40!red}#1}}
\newcommand{\tr}[1]{\todo[inline,color=blue!40]{{\textbf{TR:}~}#1}}
\newcommand{\TR}[1]{{\color{lightblue!40!blue}#1}}
\newcommand{\ar}[1]{\todo[inline,color=green!40]{{\textbf{AR:}~}#1}}
\newcommand{\AR}[1]{{\color{lime!40!green}#1}}

\title{Clustering Multivariate Ordinal Data}


\author{Thomas \textsc{Michel}, Théo \textsc{Rudkiewicz} and Ali \textsc{Ramlaoui}}



\begin{document}
\maketitle

% \tableofcontents

%\section{TODOs}
%\begin{itemize}
%    \item For the experiments:
%    \begin{itemize}
%        \item Test the robustness of the algorithm when facing outliers or skewed distributions.
%        \item Use noisy ordinal data (add discrete noise on a few features).
%        \item Test scalability of the algorithm (number of variables and data points).
%    \end{itemize}
%\end{itemize}

\section{Introduction}
The exploration of hidden structures within datasets is a crucial task for data scientists, and clustering serves as a valuable tool in this endeavor. Mixture models have emerged as a standard approach for clustering due to their capacity to provide a well-defined mathematical framework for parameter estimation and model selection. These models, instrumental in determining the number of clusters, not only encapsulate classical geometric methods but also find successful application in diverse practical scenarios.

In the realm of model-based clustering, the classification of data hinges on the availability of a suitable probability distribution tailored to the nature of the data at hand—be it numerical, rankings, functional, or categorical. Notably, ordinal data, where categories possess a specific order, represent a common occurrence, especially in fields like marketing where product evaluations are solicited through ordinal scales. Despite their prevalence, ordinal data have received comparatively less attention in the context of model-based clustering. Often, practitioners resort to transforming ordinal data into quantitative or nominal formats to align with readily applicable distributions, neglecting valuable order information.

This paper explores the less-explored domain of model-based clustering for ordinal data, specifically focusing on ordinal data derived from ordered categories. Ordinal data find widespread application in fields such as social sciences, psychology, marketing, healthcare, and more. They enable researchers to capture nuanced information, such as preferences, attitudes, or severity levels, in cases where continuous measures are neither significant nor possible. For example, when assessing tumor severity, the precise size may not be as crucial as the current state of development of the disease as it is assessed by specialists. The use of ordinal data enriches the comprehension of subjective opinions, behaviors, and hierarchical relationships across diverse research contexts. Over the years, various approaches have been proposed to define probability distributions for ordinal data, including modeling cumulative probabilities, constraining multinomial models to reflect ordinality, assuming ordinal data as discretization of continuous latent variables, and constructing distributions to meet specific properties.

Among these approaches, the work by \cite{biernacki2016model}, studied in the context of this project, delves into an original strategy—modeling the hypothetical data generating process for ordinal data. While this general principle has found success in ranking data scenarios, the distinction in the data generating process becomes apparent for ordinal data. In this context, a search algorithm, specifically the binary search algorithm, emerges as a fitting choice, respecting the ordinal nature of data through comparisons without necessitating links to nominal or continuous distributions.

The proposed model, parameterized with a position parameter (modal category) and a precision parameter, exhibits desirable properties such as a unique mode, probability distribution decrease on either side of the mode, and the flexibility to accommodate uniform or Dirac distributions. Maximum likelihood estimation using an EM algorithm is employed, leveraging the binary search algorithm's latent variable interpretation. While combinatorial complexity limits straightforward estimation for models based on latent Gaussian variables, the proposed approach remains tractable for ordinal data with up to eight categories—a common scenario for most ordinal variables.

In this project, we aim to replicate and build upon the findings of \cite{biernacki2016model}. We re-implemented their suggested probabilistic model, parameter estimation method, and model-based clustering algorithm in Python. Drawing inspiration from their approach, we propose an alternative probabilistic model with similar properties. The goal is to address computational limitations, enabling the clustering of more extensive datasets with potentially more categories than the previous method allows. We also present a preliminary analysis of this new method to justify the decreased computational cost of estimating parameters for this model. Additionally, we  test \cite{biernacki2016model}'s approach on real-world datasets and compare it to the proposed approach, along with baseline models. This is done on different datasets of multiple nature in order to check whether the proposed methods are successful in these settings in practice and what their advantages are. The ultimate goal is to check whether the gains are significant against methods that are not adapted for ordinal datasets, in order to decide whether these approaches are interesting to use in general as a default method of choice for this type of variable.
% \tm {Explain better what we do for the experiments}


\section{Method}
The approach taken in this report follows the one proposed by \cite{biernacki2016model}. It involves defining a probabilistic model based on the observation of categorical data. More precisely, the authors introduce a Stochastic Binary Search process that leads to the selection of a category. 

For this particular model, we need to make the assumption that:
\begin{itemize}
    \item The categories must be well-ordered (ordinal data): The categories are linearly ordered, and each non-empty subset contains the least element. This implies that any element can be compared to any other, and we can enumerate all the categories in increasing order.
    \item The set of categories is finite. This simplifies the previous assumption to the existence of a linear ordering. This assumption is necessary to ensure that the stochastic search terminates after a fixed number of steps, implying a finite number of possible runs of the search.
\end{itemize}
We will first define the binary ordinal search (BOS) model in the univariate case. The multivariate variant can be deduced by applying a similar search to each of the features independently. Then, we will discuss how to estimate the parameters of the model from a set of samples following the general method of the Expectation-Maximization algorithm (EM). Finally, we will show how the original authors extend EM to cluster ordinal data.

\subsection{Probabilistic model}
\paragraph{Stochastic Binary Ordinal Search} The BOS model is inspired by a standard binary search with added noise in the comparison. Consequently, the algorithm may at times misidentify the next subset for the search, ultimately causing it to overlook the sought-after value.

The stochastic binary ordinal search unfolds as follows: Let $m$ be the number of categories. For simplicity, we denote the categories using unique values from $1$ to $m$. Then, for at most $m-1$ steps, we perform the following three operations. At step $j$, we start with a subset of all the categories, denoted as $e_j\subseteq \{1,...,m\}$.

\begin{enumerate}
    \item Sample a breakpoint $y_j$ uniformly in $e_j$.
    \item Draw an accuracy indicator $z_j$ from a Bernoulli distribution with parameter $\pi$. A value of $z_j=1$ indicates that the comparison is perfect, and the next step will be computed optimally. A value of $z_j=0$ implies a blind comparison at the next step.
    \item Determine the new subset $e_{j+1}$ for the next iteration. Firstly, split the subset into three intervals, namely $e_j^-$, $e_j^=$, and $e_j^+$. $e_{j+1}$ will be chosen among these intervals. If the comparison is blind ($z_j=0$), randomly select the interval with a probability proportional to its size. Alternatively, if $z_j=1$ and the comparison is perfect, select the interval containing $\mu$ (or, by default, the one closest to it).
\end{enumerate}
After $m-1$ steps, the resulting interval contains a single value, which is the observed result $e_m={x}$ of the BOS model.

For data with multiple ordinal features, the samples are generated independently with different parameters $\mu$ and $\pi$ and possibly different numbers of categories for each feature.

\paragraph{Mixture model} 
The BOS model can be extended to a mixture model by considering multiple BOS models with different parameters. In this case, the data is generated by first sampling a cluster $k$ from a multinomial distribution with parameter $\alpha$ and then sampling the data from the BOS model with parameters $\mu_k$ and $\pi_k$.

We can also consider a multivariate version of the BOS model in addition to the mixture case where we have a BOS model for each feature independently. Each dimension is then concatenated to get the multivariate dataset that follows a multivariate BOS distribution.

Therefore, for $p$ clusters, and $d$ features, the parameters of the model are $\alpha \in \mathbb{R}^p$, $\mu \in \mathbb{R}^{p\times d}$ and $\pi \in \mathbb{R}^{p\times d}$
Since the features are independent, the probability of observing a sample $x$ knowing it belongs to cluster $k$ is given by:
\begin{equation}
    \mathbb{P}(x | \alpha_k, \mu_{k}, \pi_k) = \prod_{j=1}^d \mathbb{P}(x_j | \alpha_k, \mu_{kj}, \pi_{kj})
\end{equation}

\begin{figure}[htbp]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tikzpicture}
  \node[obs]                               (xi) {$x_i$};
  \node[latent, left=of xi]               (em) {$e_m$};
  \node[latent, below=of em]               (zm1) {$z_{m-1}$};
  \node[latent, left=of em]               (ym1) {$y_{m-1}$};
  \node[const, left=of ym1]               (dots2) {$\ldots$};
  \node[latent, left=of dots2]               (ej) {$e_{j}$};
  \node[latent, below=of ej]               (zj) {$z_{j}$};
  \node[latent, left=of ej]               (yj) {$y_{j}$};
  \node[const, left=of yj]               (dots1) {$\ldots$};
  \node[latent, left=of dots1]               (e2) {$e_2$};
  \node[latent, below=of e2]               (z1) {$z_1$};
  \node[latent, left=of e2]               (y1) {$y_1$};
  \node[latent, left=of y1]               (e1) {$e_1$};
  \node[const, above=of ej]    (mu) {$\mu$};
  \node[const, below=of zj, yshift=-.25cm]  (pi) {$\pi$};

  \edge{e1}{y1};
  \edge{y1}{e2};
  \edge{z1}{e2};
  \edge{e2}{dots1};
  \edge{dots1}{yj};
  \edge{yj}{ej};
  \edge{zj}{ej};
  \edge{ej}{dots2};
  \edge{dots2}{ym1};
  \edge{ym1}{em};
  \edge{zm1}{em};
  \edge{em}{xi};
  \edge {mu} {e2, ej, em};
  \edge {pi} {z1,zj, zm1};

  \plate[inner sep=.25cm] {} {(xi)(e1)(zm1)(z1)} {$i=1, \ldots, n$};
\end{tikzpicture}
\end{adjustbox}
\caption{Graphical model of the stochastic Binary Ordinal Search.}
\label{fig:graphical_model}
\end{figure}

%\ar{Do a figure for the Mixture case}
%\tm{Maybe, although it does not give much more information since it can be trivially derived from this one. Good job for the figure by the way}
%\ar{Agreed, and probably too complicated to do anyways}


\subsection{Parameter estimation}
\paragraph{EM algorithm.} In the univariate case, the underlying BOS distribution of a data sample $(x_1, \ldots, x_n)$ can be estimated using the Expectation-Maximization (EM) algorithm. The main idea behind EM is to iteratively compute the parameters $(\mu, \pi)$ of the distribution to maximize the log-likelihood until convergence when the model is explained by latent variables that are not observed with the data as it is the case with the BOS distribution \citep{biernacki2016model}.
\begin{enumerate}
    \item Initialization: The parameters of the distribution are initialized using a pre-defined heuristic. In our case, we initialize $(\mu^{(0)}, \pi^{(0)})$ randomly in their respective range.
    \item Expectation step: The latent variables $(c_i)_{i \in \{1, \ldots, n\}}$ are estimated using the current approximation of the parameters $(\mu^{(t)}, \pi^{(t)})$. This consists in computing the posterior probability of every latent variable using the conditional probability formula for all $i\in \{1, \ldots, ..., n\}$
    \begin{equation}
        \mathbb{P}(c_i | x_i, \mu^{(t)}, \pi^{(t)}) = \frac{\mathbb{P}(c_i, x_i| \mu^{(t)}, \pi^{(t)})}{\mathbb{P}(x_i| \mu^{(t)}, \pi^{(t)})}
    .\end{equation}
    This can be computed recursively for the different latent variables using their prior distribution (BOS model):
    \begin{equation}
        \mathbb{P}(e_{j+1}| \mu^{(t)}, \pi^{(t)}) = \sum_{e\in \mathcal{P}(\{1, \ldots, m\})} \mathbb{P}(e_{j+1}|e_j=e,  \mu^{(t)}, \pi^{(t)}) \mathbb{P}(e_j=e| \mu^{(t)}, \pi^{(t)})
    .\end{equation}
    The intermediate conditional probability is obtained by conditioning on the different values that can be taken by $y_j$ and $z_j$ when $e_j=e$:
    \begin{align}
    \mathbb{P}(e_{j+1}|e_j=e, \mu^{(t)}, \pi^{(t)}) &= \sum_{y_j \in e} \mathbb{P}(e_{j+1}|e_j=e, y_j, \mu^{(t)}, \pi^{(t)})\mathbb{P}(y_j|e_j=e) \\
    &\begin{aligned}
        = &\sum_{y_j \in e} (\pi^{(t)}\mathbb{P}(e_{j+1}|e_j, y_j, z_j=1, \mu^{(t)}, \pi^{(t)}) \\ 
        & + (1-\pi^{(t)})\mathbb{P}(e_{j+1}|e_j, y_j, z_j=0, \mu^{(t)}, \pi^{(t)}))\mathbb{P}(y_j|e_j=e)
    .\end{aligned}
    \end{align}
    The components of the sum can be more easily computed by distinguishing between the cases where $z_j=0$ and $z_j=1$ with closed-forms expressions from the prior. \\
    Moreover, for $y_j$, the posterior distribution can also be obtained using the following expression where we also condition on $e_j$:
    \begin{equation}
    \mathbb{P}(y_j| \mu^{(t)}, \pi^{(t)}) = \sum_{e\in \mathcal{P}(\{1, \ldots, m\}} \mathbb{P}(y_j|e_j=e, \mu^{(t)}, \pi^{(t)})\mathbb{P}(e_j=e| \mu^{(t)}, \pi^{(t)})
    .\end{equation}
    For $z_j$, $\mathbb{P}(z_j| \mu^{(t)}, \pi^{(t)})$ is a Bernoulli variable of probability $\pi^{(t)}$ so it can easily be expressed:
    \begin{equation}
    \mathbb{P}(z_j| \mu^{(t)}, \pi^{(t)}) = \begin{cases}
        \pi^{(t)}\quad &\text{if } z_j = 1\\
        1 - \pi^{(t)}\quad &\text{if } z_j = 0
        \end{cases}
    .\end{equation}
    %\ar{How does this allow to derive $\mathbb{P}(c_{ij}, x_i| \mu, \pi)?$. Ok}
    Note that since in the BOS model $e_m$ is identified with $x_i$, we get the following joint distribution for all the latent variables $c_i$ with the observation $x_i$: 
    \begin{equation}
       \mathbb{P}(c_i,x_i| \mu^{(t)}, \pi^{(t)}) = \mathbb{P}(e_m|c_i, \mu^{(t)}, \pi^{(t)})\mathbb{P}(c_i| \mu^{(t)}, \pi^{(t)})
    .\end{equation}
    \item Maximization step: During this step, the next iteration of the parameters $(\mu^{(t+1)}, \pi^{(t+1)})$ that maximizes the log-likelihood of observing the data taking into account the latent-variables probabilities computed during the Expectation step. As proposed by \citet{biernacki2016model}, only $\pi$ is updated and $\mu$ is fixed during the entire algorithm. This is done for every possible value of $\mu$ and the value that maximizes the log-likelihood is chosen. \\
    The new value of $\pi$ is given after computing the maximizer of the expectation of the log-likelihood in $\pi$:
    \begin{equation}
    \pi^{(t+1)} = \frac{\sum_{i=1}^N \sum_{j=1}^{m-1} \mathbb{P}(z_{ij}=1|x_i, \mu^{(t)}, \pi^{(t)})}{n(m-1)}
    .\end{equation}
    % \begin{proof}
    % \ar{Add the proof for this formula?}
    % \tm{Yes. I need to check it but not sure there is much to it. Théo opinion ?}
    % \end{proof}
    \footnote{The authors of the paper did not mention how they computed this formula. We computed it by marginalizing on every possible run of the binary search algorithm, leading to high computational cost.

    We use the following expression:
    \begin{align}
        \Pr(z_{ij}=1|x_i, \mu, \pi) &= \sum_{c_i} \Pr(z_{ij}=1, c_i|x_i, \mu, \pi) \\
        &= \sum_{c_i} \Pr(z_{ij}=1|c_i, x_i, \mu, \pi)\Pr(c_i|x_i, \mu, \pi) \\
        &= \sum_{c_i} \Pr(z_{ij}=1|c_i) \frac{\Pr(c_i, x_i|\mu, \pi)}{\Pr(x_i|\mu, \pi)} \\
        &= \frac{1}{\Pr(x_i|\mu, \pi)} \sum_{c_i} \Pr(z_{ij}=1|c_i) \Pr(c_i, x_i|\mu, \pi) 
    \end{align}
    And then:
    \begin{align}
        \sum_{j=1}^{m-1} \Pr(z_{ij}=1|x_i, \mu, \pi) &= \sum_{j=1}^{m-1} \frac{1}{\Pr(x_i|\mu, \pi)}  \sum_{c_i} \Pr(z_{ij}=1|c_i) \Pr(c_i, x_i|\mu, \pi) \\
        &= \frac{1}{\Pr(x_i|\mu, \pi)} \sum_{c_i} \Pr(c_i, x_i|\mu, \pi)  \sum_{j=1}^{m-1} \Pr(z_{ij}=1|c_i)
    \end{align}
    }

\end{enumerate}

\paragraph{AECM algorithm.} Similarly to the EM algorithm, Alternating Expectation-Conditional Maximization (AECM) \citep{meng1997algorithm} is separated in two steps. However, in this case, we consider multivariate ordinal data with different possible distributions (clusters) priors for the data. This is done, just like for the Gaussian Mixture Model case, using latent variables $w_{ik}$ which describe whether the data $x_i$ belongs to the cluster $k$ or not, and parameters $(\alpha_k)_{k\in \{1, \ldots, p\}}$ which describe the probability of belonging to each cluster. 
\begin{enumerate}
    \item Expectation step: In this case, the expectation step consists in just computing the probability for every data point to belong to each cluster:
    \begin{equation}
        \mathbb{P}(w_{ik}=1|x_i, \alpha^{(t)}, \mu^{(t)}, \pi^{(t)}) = \frac{\alpha_k^{(t)}\mathbb{P}(x_i|w_{ik}=1, \mu_k^{(t)}, \pi_k^{(t)})}{\sum_{l=1}^p\alpha_l^{(t)}\mathbb{P}(x_i|w_{il}=1, \mu_l^{(t)}, \pi_l^{(t)})}.
    \end{equation}
    \item Maximization step: The parameters are updated using the new expected values for belonging to the different cluster. Since there are two groups of latent variables, the clusters variables $\alpha_k^{(t)}$ are updated first to maximize the log-likelihood:
    \begin{equation}
    \alpha_k^{(t+1)} = \frac{1}{n} \sum_{i=1}^n \mathbb{P}(w_{ik}=1|x_i, \alpha^{(t)}, \mu^{(t)}, \pi^{(t)}).
    \end{equation}
    And then the parameters $(\mu_k^{(t+1)}, \pi_k^{(t+1)})$ are updated after using an EM algorithm in the univariate case for every cluster $k$ and for every dimension of the multivariate variables independently using the data on the corresponding dimension.
\end{enumerate}

\input{bos_model_appendix.tex}

\input{god_model}

%EM on the probabilistic model + algorithm
\section{Experiments}
In this section, we try to evaluate the performance of the two models on different datasets. We first present the experimental setup and the datasets used for the experiments. We then present the results obtained for the estimation algorithms on synthetic data and on real-life datasets. We finally discuss the results obtained and the relevance of the models. 

The goal of these experiments is to compare the two models but also to individually test their ability to cluster ordinal datasets and to check whether they are able to generalize to real-life datasets. 

\subsection{Experimental setup}
\paragraph{Parameters of the experiment}
In this section, we propose to test the BOS model on synthetic data and on real-life datasets. 
To do so, we also use simple clustering algorithms to compare the performance of the BOS model on data that is adapted (ordinal) with algorithms that are not designed for this kind of data such as K-Means \citep{macqueen1967some} and Gaussian Mixture Models \citep{reynolds2009gaussian}.

We also test the AECM algorithm for the BOS model with both a random initialization of the parameters and an initialization of the parameters using the K-Means algorithm. \\ \\
Runtimes are also measured on the same machine for all the algorithms to compare their efficiency.
\paragraph{Dataset.} One of the main goal of the experiments is to test the ability of the models to generalize to real-life datasets. We therefore propose to test the illustrated methods on real world datasets to check the usefulness of the models on different real-life situations. Since the algorithm is specifically designed for ordinal observations, the datasets need to be adapted for the task. One way to apply to obtain real-life datasets is to quantize continuous datasets of observations that can be categorized (e.g. movies, store products, species...) \citep{skubacz2000quantization}. Another interesting approach could be to test the models on tasks that they were not specifically designed for. This could allow seeing how they can generalize and whether they are applicable to a broader class of problems. We therefore propose to test the ability to cluster observations of binary features into different animal species.
\paragraph{Zoo Dataset.} The zoo dataset consists of multiple features describing $101$ different animals, with most of them being binary variables associated to a characteristic of the animal (hair, feathers, eggs, milk, \ldots) \citep{misc_zoo_111}. Every animal belongs to one of $6$ classes. 
\paragraph{Car Evaluation Dataset.} The car evaluation dataset consists of multiple features describing $1728$ different cars, with most of them being ordinal variables associated to a characteristic of the car (buying price, maintenance price, number of doors, \ldots) \citep{misc_car_evaluation_19}. Every car belongs to one of $4$ classes.
\paragraph{Hayes-Roth Dataset.} The Hayes-Roth dataset consists of multiple features describing $132$ different persons, with most of them being binary variables associated to a characteristic of the person (has a PhD, is married, is a professional, \ldots) \citep{misc_hayes_roth_44}. Every person belongs to one of $3$ classes.
\paragraph{Caesarian Dataset.} The Caesarian dataset is a dataset describing $80$ different patients with multiple features associated to the patient (age, delivery number, delivery time, blood pressure, \ldots) \citep{misc_caesarian_section_classification_dataset_472}. Every patient belongs to one of $2$ classes. \\ \\
The advantage of these datasets is that they are small enough to be able to compute the exact likelihood of the data given the model and the parameters. This allows to check whether the models are able to correctly fit the data.

\paragraph{Evaluation method.} For most of the evaluation datasets, we will use classification tasks to check the ability to cluster with respect to pre-existing classes. This allows the evaluation framework to be easier to define and to compare different methods more easily. 

In order to correctly associate the predicted clusters with the true clusters, we need to define a strategy that matches each predicted clusters with a true cluster number which will minimize a given criterion.
In order to do so, we propose two methods:
\begin{itemize}
    \item The first one and consists in sorting the histograms of the predicted clusters and the true clusters and then matching the two sorted lists by assigning the predicted clusters to the true cluster in the same sorted order.
    
    This method is naive because it does not take into account the distribution of the real clusters according to the true labels for the matching.
    
\item The second method consists in solving the Assignment Problem \citep{kuhn1955hungarian} with the cost matrix being the distance between the histograms of the predicted clusters and the true clusters. This method takes into account the distribution of the real clusters according to the true labels for the matching. We can easily solve it using any Optimal Transport algorithm (or by defining the Linear Programming problem and solving it using an LP solver).
\end{itemize}
Figure \ref{fig:assignment_methods} in appendix \ref{appendix:metrics_real} shows that the optimal matching when considering the assignment matrix is a better choice in the case of the Zoo dataset for example and that the classes in the predicted distribution are assigned to the correct true class with respect to their proportions.


The evaluation metrics used to compare the different models are the F1-score, and the Accuracy score in the cases where the datasets are suited for classification and the Wasserstein distance and the Adjusted Rand Index (ARI). % and the Normalized Mutual Information (NMI). 
\begin{itemize}
    \item The F1-score is the harmonic mean of the precision and the recall for classification problems.
    \item The Wasserstein distance is a measure of the distance between two probability distributions \citep{ramdas2017wasserstein}. 
        It measures the cost of transforming one distribution into the other using the optimal transport plan which in this case is the matching obtained as described above.
        \begin{equation}
            W(\hat{y}, y) = \min_{\gamma \in \Gamma(\hat{y}, y)} \sum_{i, j} \gamma_{i, j} \norm{i - j}
        ,\end{equation}
    where $\Gamma(\hat{y}, y)$ is the set of all possible matchings between the predicted clusters and the true clusters and $\gamma_{i, j}$ is the probability of matching the predicted cluster $i$ with the true cluster $j$ (i.e. it is the proportion of the samples in the predicted cluster $i$ that are in the true cluster $j$) for the matching.
\item The ARI is a measure of the similarity between two clusterings of the same dataset. It is a function that outputs a value between -0.5 and 1, where 1 means that the two clusterings are identical, 0 means that the two clusterings are independent (random) and -0.5 means that the two clusterings are as different as possible. The ARI is symmetric and therefore does not take into account the order of the clusters \citep{steinley2004properties}. 
    \begin{equation}
    \text{ARI}(\hat{y}, y) = \frac{\sum_{i, j} \binom{n_{i, j}}{2} - \left[\sum_i \binom{\hat{n}_i}{2} \sum_j \binom{n_j}{2}\right] / \binom{n}{2}}{\frac{1}{2} \left[\sum_i \binom{\hat{n}_i}{2} + \sum_j \binom{n_j}{2}\right] - \left[\sum_i \binom{\hat{n}_i}{2} \sum_j \binom{n_j}{2}\right] / \binom{n}{2}}
    ,\end{equation}
where $n_{i, j}$ is the number of samples that are in the predicted cluster $i$ and in the true cluster $j$, $\hat{n}_i$ is the number of samples in the predicted cluster $i$ and $n_j$ is the number of samples in the true cluster $j$.
\end{itemize}

\subsection{Estimation methods}
In this section, we present the results obtained after running the AECM estimation algorithms for different parameters used to generate data from the BOS distribution and from the GOD model in order to compare the different estimators for their respective distributions. We will then also try the different models on real-life datasets in order to check how they perform for clustering on real data.

\paragraph{BOS distribution.} We first test the AECM algorithm on the BOS distribution with experiments similar to \cite{biernacki2016model} in order to confirm that our implementation is coherent with the algorithm proposed and yields results that are close. We generate data from the BOS distribution with different parameters and then run the AECM algorithm on the generated data to estimate the parameters. We then compare the estimated parameters with the true parameters using the $L_1$ distance between the two vectors. We repeat this process multiple times for different parameters and average the results to obtain the results presented in Table~\ref{tab:results_bos} in Appendix \ref{appendix:metrics_synth}.

\paragraph{GOD model.} We then test the AECM algorithm on the GOD model with experiments similar to \cite{biernacki2016model} in order to confirm that the estimation algorithms and the estimators proposed are able to estimate the parameters generated from a GOD distribution. We use data generated from the GOD model with similar parameters as for the BOS distribution. 
The results are presented in Table~\ref{tab:results_god} in the appendix section.

\subsection{Experiments with real-life datasets}
In this section, we present the results obtained after running the AECM algorithm on the real-life datasets presented above. We then compare the results obtained with the BOS model with the results obtained with the GOD model and with the results obtained with the K-Means algorithm and the Gaussian Mixture Models. We also compare the results obtained with the two different methods to match the predicted clusters with the true clusters. The results are presented in Table~\ref{tab:results_real} in the appendix \ref{appendix:metrics_real}.


% \ar{Add final table when possible, make size smaller or round to 2 digits}
% \tm{J'ai mis un exemple pour rétrécir la table avec adjustbox dans la version actuelle du tableau}
We notice that although K-Means allows to significantly reduce the runtime of both the BOS and the GOD models estimations, it does not necessarily increase the clustering score and the classification score. The BOS model, because of its complexity, is also the longest to run but seems to be competitive with the other models on most datasets.

In order to get a better idea of the differences between the clustering methods, we also plot t-SNE visualizations \citep{van2008visualizing} for different datasets and the multiple models. Figures \ref{fig:tsne_zoo} in appendix \ref{sec:appendix_tsne} shows the plots of all the features and the associated true labels and clusters.

Histograms and assignment matrices of some datasets are provided in appendix~\ref{sec:appendix_hist} and appendix~\ref{sec:appendix_assign} in order to get a better understanding of the different assignments obtained in these settings.

% \subsection{"experiment with a modification of the method"}


\section{Conclusion}
In this study, we analyzed model-based clustering for ordinal data, with a specific focus on the Binary Ordinal Search (BOS) and a proposed alternative we called Globally Ordinal Distribution (GOD) models. We aimed to understand and evaluate their efficiency in clustering and classifying ordinal data compared to more traditional methods like K-Means and Gaussian Mixture Models. Our exploration spanned both synthetic and real-world datasets, providing a comprehensive view of the models' performance in various scenarios.

The experiments on synthetic data confirmed the theoretical foundations of the BOS and GOD models. When parameters were known, both models showed an ability to recover the underlying structure of the generated data. Particularly, the BOS model, despite its computational intensity due to its design, performed well in clustering tasks, highlighting its potential for applications with ordinal data. The GOD model, with its more manageable computational requirements, also demonstrated promising results, making it a practical alternative for larger datasets.

When applied to real-world datasets, the results were more nuanced. While both BOS and GOD models performed competitively in certain scenarios, they did not universally outperform the traditional methods. This suggests that while specialized ordinal models are interesting, especially in scenarios where the ordinal nature of data is pronounced, they are not a default solution. It is essential to consider the specific characteristics of the dataset and the computational resources available when choosing the appropriate clustering method.

Different visualizations also provide further insights into how the models partition the data space. They revealed that while the clusters identified by the BOS and GOD models often made intuitive sense, they sometimes differ significantly from those identified by K-Means and Gaussian Mixture Models. This highlights the different assumptions and approaches these models take when learning the structure within data.

In conclusion, the study reaffirms the potential of model-based clustering for ordinal data, particularly highlighting the BOS and GOD models as valuable tools. However, it also demonstrates that the choice of model should be informed by both the nature of the data and the practical constraints of the problem at hand. Further research could explore further refinements to these models, more extensive comparisons with other methods, and applications to a broader range of real-world scenarios.

\section{Contribution statement}
This project reflects a collaborative effort where all three students, Ali \textsc{Ramlaoui}, Thomas \textsc{Michel} and Théo \textsc{Rudkiewicz}, made equal and substantial contributions. 
We give here the main but not exclusive focus of each one:
\begin{itemize}
    \item Ali \textsc{Ramlaoui} focused on the experiments and the implementation of the AECM algorithm.
    \item Thomas \textsc{Michel} implemented the BOS model and the EM algorithm for the BOS model.
    \item Théo \textsc{Rudkiewicz} developed the GOD model and the algorithm associated with it.
\end{itemize}

\bibliography{references}

\appendix

\section{AECM tests for BOS and GOD distributions}
\label{appendix:metrics_synth}

\begin{table}[H]
\centering
\begin{minipage}{.48\columnwidth}
\centering
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{lllllrrr}
\toprule
 &  &  &  &  & $\Delta \alpha$ & $\Delta \mu$ & $\Delta \pi$ \\
Init. & $n$ & $n_{clusters}$ & $d$ & $n_{cats}$ &  &  &  \\
\midrule
\multirow[t]{16}{*}{K-Means} & \multirow[t]{8}{*}{50} & \multirow[t]{4}{*}{3} & \multirow[t]{2}{*}{3} & 2 & 0.143 & 0.167 & 0.295 \\
 &  &  &  & 3 & 0.135 & 0.667 & 0.116 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.090 & 0.200 & 0.172 \\
 &  &  &  & 3 & 0.083 & 0.333 & 0.105 \\
\cline{3-8} \cline{4-8}
 &  & \multirow[t]{4}{*}{5} & \multirow[t]{2}{*}{3} & 2 & 0.099 & 0.667 & 0.337 \\
 &  &  &  & 3 & 0.142 & 0.778 & 0.356 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.099 & 0.400 & 0.210 \\
 &  &  &  & 3 & 0.125 & 0.400 & 0.157 \\
\cline{2-8} \cline{3-8} \cline{4-8}
 & \multirow[t]{8}{*}{250} & \multirow[t]{4}{*}{3} & \multirow[t]{2}{*}{3} & 2 & 0.111 & 0.167 & 0.344 \\
 &  &  &  & 3 & 0.021 & 0.444 & 0.206 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.152 & 0.200 & 0.162 \\
 &  &  &  & 3 & 0.043 & 0.267 & 0.106 \\
\cline{3-8} \cline{4-8}
 &  & \multirow[t]{4}{*}{5} & \multirow[t]{2}{*}{3} & 2 & 0.149 & 0.667 & 0.423 \\
 &  &  &  & 3 & 0.315 & 0.444 & 0.246 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.134 & 0.400 & 0.290 \\
 &  &  &  & 3 & 0.174 & 0.467 & 0.129 \\
\cline{1-8} \cline{2-8} \cline{3-8} \cline{4-8}
\multirow[t]{16}{*}{Random} & \multirow[t]{8}{*}{50} & \multirow[t]{4}{*}{3} & \multirow[t]{2}{*}{3} & 2 & 0.112 & 0.167 & 0.074 \\
 &  &  &  & 3 & 0.085 & 0.222 & 0.025 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.011 & 0.000 & 0.043 \\
 &  &  &  & 3 & 0.039 & 0.067 & 0.038 \\
\cline{3-8} \cline{4-8}
 &  & \multirow[t]{4}{*}{5} & \multirow[t]{2}{*}{3} & 2 & 0.058 & 0.167 & 0.081 \\
 &  &  &  & 3 & 0.073 & 0.111 & 0.079 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.095 & 0.300 & 0.130 \\
 &  &  &  & 3 & 0.068 & 0.067 & 0.117 \\
\cline{2-8} \cline{3-8} \cline{4-8}
 & \multirow[t]{8}{*}{250} & \multirow[t]{4}{*}{3} & \multirow[t]{2}{*}{3} & 2 & 0.095 & 0.000 & 0.035 \\
 &  &  &  & 3 & 0.018 & 0.222 & 0.007 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.031 & 0.000 & 0.016 \\
 &  &  &  & 3 & 0.017 & 0.067 & 0.019 \\
\cline{3-8} \cline{4-8}
 &  & \multirow[t]{4}{*}{5} & \multirow[t]{2}{*}{3} & 2 & 0.037 & 0.167 & 0.022 \\
 &  &  &  & 3 & 0.057 & 0.222 & 0.037 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.044 & 0.000 & 0.022 \\
 &  &  &  & 3 & 0.020 & 0.067 & 0.052 \\
\cline{1-8} \cline{2-8} \cline{3-8} \cline{4-8}
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Results of the experiments for the AECM algorithm no synthetic data with the BOS distribution. The parameters are the number of samples $n$, the number of clusters $n_{clusters}$, the dimension $d$ and the number of categories $n_{cats}$. The deltas are the average of the $L_1$ distances between the true and estimated parameters after applying optimal transport to find the correct clusters.}
\label{tab:results_bos}
\end{minipage} \hspace{.02\columnwidth}%
\begin{minipage}{.48\columnwidth}
\centering
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{lllllrrr}
\toprule
 &  &  &  &  & $\Delta \alpha$ & $\Delta \mu$ & $\Delta \pi$ \\
Init. & $n$ & $n_{clusters}$ & $d$ & $n_{cats}$ &  &  &  \\
\midrule
\multirow[t]{16}{*}{K-Means} & \multirow[t]{8}{*}{50} & \multirow[t]{4}{*}{3} & \multirow[t]{2}{*}{3} & 2 & 0.103 & 0.167 & 0.166 \\
 &  &  &  & 3 & 0.132 & 0.444 & 0.095 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.063 & 0.300 & 0.088 \\
 &  &  &  & 3 & 0.074 & 0.067 & 0.020 \\
\cline{3-8} \cline{4-8}
 &  & \multirow[t]{4}{*}{5} & \multirow[t]{2}{*}{3} & 2 & 0.082 & 0.333 & 0.207 \\
 &  &  &  & 3 & 0.149 & 0.778 & 0.118 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.102 & 0.300 & 0.122 \\
 &  &  &  & 3 & 0.194 & 0.400 & 0.089 \\
\cline{2-8} \cline{3-8} \cline{4-8}
 & \multirow[t]{8}{*}{250} & \multirow[t]{4}{*}{3} & \multirow[t]{2}{*}{3} & 2 & 0.093 & 0.167 & 0.152 \\
 &  &  &  & 3 & 0.079 & 0.222 & 0.076 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.088 & 0.200 & 0.071 \\
 &  &  &  & 3 & 0.121 & 0.267 & 0.073 \\
\cline{3-8} \cline{4-8}
 &  & \multirow[t]{4}{*}{5} & \multirow[t]{2}{*}{3} & 2 & 0.083 & 0.833 & 0.211 \\
 &  &  &  & 3 & 0.104 & 0.889 & 0.125 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.097 & 0.300 & 0.110 \\
 &  &  &  & 3 & 0.113 & 0.400 & 0.064 \\
\cline{1-8} \cline{2-8} \cline{3-8} \cline{4-8}
\multirow[t]{16}{*}{Random} & \multirow[t]{8}{*}{50} & \multirow[t]{4}{*}{3} & \multirow[t]{2}{*}{3} & 2 & 0.063 & 0.167 & 0.104 \\
 &  &  &  & 3 & 0.113 & 0.333 & 0.084 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.050 & 0.100 & 0.057 \\
 &  &  &  & 3 & 0.075 & 0.267 & 0.053 \\
\cline{3-8} \cline{4-8}
 &  & \multirow[t]{4}{*}{5} & \multirow[t]{2}{*}{3} & 2 & 0.058 & 0.333 & 0.145 \\
 &  &  &  & 3 & 0.177 & 0.667 & 0.092 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.083 & 0.400 & 0.112 \\
 &  &  &  & 3 & 0.153 & 0.467 & 0.056 \\
\cline{2-8} \cline{3-8} \cline{4-8}
 & \multirow[t]{8}{*}{250} & \multirow[t]{4}{*}{3} & \multirow[t]{2}{*}{3} & 2 & 0.032 & 0.167 & 0.093 \\
 &  &  &  & 3 & 0.197 & 0.222 & 0.042 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.014 & 0.200 & 0.050 \\
 &  &  &  & 3 & 0.072 & 0.133 & 0.017 \\
\cline{3-8} \cline{4-8}
 &  & \multirow[t]{4}{*}{5} & \multirow[t]{2}{*}{3} & 2 & 0.052 & 0.167 & 0.118 \\
 &  &  &  & 3 & 0.098 & 0.889 & 0.146 \\
\cline{4-8}
 &  &  & \multirow[t]{2}{*}{5} & 2 & 0.058 & 0.400 & 0.085 \\
 &  &  &  & 3 & 0.164 & 0.400 & 0.046 \\
\cline{1-8} \cline{2-8} \cline{3-8} \cline{4-8}
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Results of the experiments for the AECM algorithm no synthetic data with the GOD model. The parameters are the number of samples $n$, the number of clusters $n_{clusters}$, the dimension $d$ and the number of categories $n_{cats}$. The deltas are the average of the $L_1$ distances between the true and estimated parameters after applying optimal transport to find the correct clusters.}
\label{tab:results_god}
\end{minipage}
\end{table}

\section{Metrics on real-world datasets}
\label{appendix:metrics_real}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Attachments/assignment_method.png}
    \caption{Illustration of the two assignment matrices from the different methods after clustering the Zoo dataset. On the left, the naive method and on the right, the optimal assignment method.
    The numbers in the matrices represent the number of samples in the predicted cluster $i$ that are in the true cluster $j$.}
    \label{fig:assignment_methods}
\end{figure}

\begin{table}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{lllllll}
\toprule
 &  & \textbf{Runtime (s)} & \textbf{F1} & \textbf{Accuracy} & \textbf{Wasserstein} & \textbf{ARI} \\
Dataset & Method &  &  &  &  &  \\
\midrule
\multirow[t]{6}{*}{\textbf{Zoo}} & \textbf{BOS Random} & 47.28 & 0.77 & 0.78 & 0.35 & \textbf{\underline{0.76}} \\
\textbf{} & \textbf{BOS K-Means} & 4.75 & \textbf{\underline{0.86}} & \textbf{\underline{0.84}} & 0.17 & 0.90 \\
\textbf{} & \textbf{GOD Random} & 52.64 & 0.87 & 0.86 & 0.23 & 0.83 \\
\textbf{} & \textbf{GOD K-Means} & 15.12 & 0.87 & 0.85 & \textbf{\underline{0.14}} & 0.90 \\
\textbf{} & \textbf{K-Means} & \textbf{\underline{0.01}} & 0.70 & 0.64 & 0.84 & 0.58 \\
\textbf{} & \textbf{Gaussian} & 0.75 & 0.79 & 0.76 & 0.33 & 0.73 \\
\cline{1-7}
\multirow[t]{6}{*}{\textbf{Car Evaluation}} & \textbf{BOS Random} & 481.93 & 0.46 & 0.40 & 0.66 & 0.02 \\
\textbf{} & \textbf{BOS K-Means} & 415.80 & 0.42 & 0.37 & 0.73 & -0.01 \\
\textbf{} & \textbf{GOD Random} & 28.95 & 0.43 & 0.40 & 0.41 & -0.04 \\
\textbf{} & \textbf{GOD K-Means} & 19.26 & 0.46 & 0.39 & 0.94 & 0.05 \\
\textbf{} & \textbf{K-Means} & \textbf{\underline{0.01}} & 0.41 & 0.34 & 0.91 & 0.01 \\
\textbf{} & \textbf{Gaussian} & 0.04 & 0.44 & 0.36 & 1.07 & 0.02 \\
\cline{1-7}
\multirow[t]{6}{*}{\textbf{Hayes-Roth}} & \textbf{BOS Random} & 307.60 & 0.37 & 0.39 & 0.25 & 0.00 \\
\textbf{} & \textbf{BOS K-Means} & 101.11 & 0.36 & 0.36 & 0.30 & -0.01 \\
\textbf{} & \textbf{GOD Random} & 11.22 & 0.37 & 0.39 & \textbf{\underline{0.25}} & -0.01 \\
\textbf{} & \textbf{GOD K-Means} & 8.49 & 0.37 & 0.36 & 0.23 & -0.01 \\
\textbf{} & \textbf{K-Means} & \textbf{\underline{0.00}} & 0.34 & 0.33 & 0.16 & -0.01 \\
\textbf{} & \textbf{Gaussian} & 0.02 & \textbf{\underline{0.45}} & \textbf{\underline{0.45}} & 0.11 & \textbf{\underline{0.07}} \\
\cline{1-7}
\multirow[t]{6}{*}{\textbf{Caesarian}} & \textbf{BOS Random} & 52.78 & 0.41 & 0.56 & 0.41 & -0.01 \\
\textbf{} & \textbf{BOS K-Means} & 35.48 & 0.53 & 0.53 & 0.05 & -0.01 \\
\textbf{} & \textbf{GOD Random} & 3.47 & 0.54 & 0.54 & \textbf{\underline{0.04}} & -0.01 \\
\textbf{} & \textbf{GOD K-Means} & 3.51 & 0.59 & 0.59 & 0.09 & 0.02 \\
\textbf{} & \textbf{K-Means} & \textbf{\underline{0.01}} & 0.56 & 0.56 & 0.09 & 0.00 \\
\textbf{} & \textbf{Gaussian} & 0.02 & \textbf{\underline{0.60}} & \textbf{\underline{0.60}} & 0.00 & \textbf{\underline{0.03}} \\
\cline{1-7}
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{
Results of the classification task for the different datasets and the proposed methods. 
The metrics are the F1-score, the accuracy, the Wasserstein distance and the adjusted rand index (ARI). 
The runtime is also reported. 
The best results for each dataset and metric are highlighted in bold and underlined. 
}
\label{tab:results_real}
\end{table}
%\tm{Penser à mettre en avant les bons résultats. Eventuellement ajouter les temps de calcul}

\section{t-SNE plots}
\label{sec:appendix_tsne}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Attachments/tsne_zoo.png}
    \caption{t-SNE visualization of the Zoo dataset with the true labels and the predicted clusters for the BOS model with random initialization, the GOD model with random initialization, the K-Means algorithm and the Gaussian Mixture Models.}
    \label{fig:tsne_zoo}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Attachments/tsne_car_evaluation.png}
    \caption{t-SNE visualization of the Car Evaluation dataset with the true labels and the predicted clusters for the BOS model with random initialization, the GOD model with random initialization, the K-Means algorithm and the Gaussian Mixture Models.}
    \label{fig:tsne_car}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Attachments/tsne_hayes-roth.png}
    \caption{t-SNE visualization of the Hayes-Roth dataset with the true labels and the predicted clusters for the BOS model with random initialization, the GOD model with random initialization, the K-Means algorithm and the Gaussian Mixture Models.}
    \label{fig:tsne_hr}
\end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{Attachments/tsne_caesarian.png}
%     \caption{t-SNE visualization of the Caesarian dataset with the true labels and the predicted clusters for the BOS model with random initialization, the GOD model with random initialization, the K-Means algorithm and the Gaussian Mixture Models.}
%     \label{fig:tsne_caesarian}
% \end{figure}

\section{Assignment Matrices}
\label{sec:appendix_assign}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Attachments/assignment_matrix_zoo.png}
    \caption{Assignment matrix for the Zoo dataset with different methods.}
    \label{fig:assign_zoo}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Attachments/assignment_matrix_hayes-roth.png}
    \caption{Assignment matrix for the Hayes-Roth dataset with different methods.}
    \label{fig:assign_hr}
\end{figure}

\section{Histograms of the different clustering}
\label{sec:appendix_hist}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Attachments/histograms_zoo.png}
    \caption{Histograms for the Zoo dataset with different methods.}
    \label{fig:hist_zoo}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Attachments/histograms_hayes-roth.png}
    \caption{Histograms for the Hayes-Roth dataset with different methods.}
    \label{fig:hist_hr}
\end{figure}

\section{GOD Model proofs}
\label{appendix:god}

\input{god_model_appendix}



\end{document}
